{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29886/29886 [00:11<00:00, 2670.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate prediction: 2.7102991367195344\n",
      "offensive prediction: 7.555377099645319\n",
      "neutral prediction: 89.73432376363515\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use Keras Tensorflow deeplearning library\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "#set random seed for the session and also for tensorflow that runs in background for keras\n",
    "set_random_seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import eli5 # for permutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import shap # for SHAP value\n",
    "from pdpbox import pdp, info_plots # flor partial plots \n",
    "\n",
    "np.random.seed(123)\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "test = pd.read_csv(\"../LSTM/input/test.csv\")\n",
    "\n",
    "#print(test.head())\n",
    "\n",
    "# Test dataset: Need only text as 'Phrase'\n",
    "test = test.rename(columns={'Text':'Phrase'})\n",
    "test = test.drop(['Date', 'Favorites', 'Retweets', 'Tweet ID'],axis=1).copy()\n",
    "#print(test.head())\n",
    "\n",
    "\n",
    "def clean_sentences(df):\n",
    "    tweets = []\n",
    "    \n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        \n",
    "        # remove non-alphabetic characters\n",
    "        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(sent))\n",
    "        \n",
    "        #remove html content\n",
    "        tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        words = word_tokenize(tweet_text.lower())\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        tweets.append(lemma_words)\n",
    "        \n",
    "    return(tweets)\n",
    "\n",
    "test_sentences = clean_sentences(test)\n",
    "\n",
    "# Getting the no of unique words and max length of a tweet available in the list of cleaned tweets\n",
    "# It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "# Build an unordered collection of unique elements.\n",
    "\n",
    "#based on train data\n",
    "unique_words = 28701\n",
    "len_max = 53\n",
    "\n",
    "# for sent in tqdm(test_sentences):\n",
    "    \n",
    "#     unique_words.update(sent)\n",
    "    \n",
    "#     if(len_max<len(sent)):\n",
    "#         len_max=len(sent)\n",
    "\n",
    "tokenizer = Tokenizer(unique_words)\n",
    "tokenizer.fit_on_texts(list(test_sentences))\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "\n",
    "#print(X_test.shape)\n",
    "\n",
    "\n",
    "# to load it again\n",
    "from keras.models import load_model\n",
    "model = load_model('lstm_model.h5',compile=False)\n",
    "\n",
    "# # save as JSON\n",
    "# json_string = model.to_json()\n",
    "\n",
    "# # save as YAML\n",
    "# yaml_string = model.to_yaml()\n",
    "\n",
    "# # model reconstruction from JSON:\n",
    "# from keras.models import model_from_json\n",
    "# model = model_from_json(json_string)\n",
    "\n",
    "# # model reconstruction from YAML:\n",
    "# from keras.models import model_from_yaml\n",
    "# model = model_from_yaml(yaml_string)\n",
    "\n",
    "# RUN MODEL on TEST DATA!\n",
    "predicted_output = model.predict(X_test, batch_size=256)\n",
    "\n",
    "# covert keras arrays\n",
    "def keras_output_sklearn(y):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for element in y:\n",
    "        result.append(np.argmax(element))\n",
    "        \n",
    "\n",
    "    return result\n",
    "\n",
    "# create pandas df with predictions \n",
    "df_test_predict = pd.DataFrame(keras_output_sklearn(predicted_output))\n",
    "\n",
    "# rename column\n",
    "df_test_predict.rename(columns={df_test_predict.columns[0]: \"result\" }, inplace = True)\n",
    "\n",
    "# replace sentiment values\n",
    "df_test_predict = df_test_predict.replace({0:'hate',1:'offensive',2:'neutral'})\n",
    "\n",
    "# normalize to percentages\n",
    "norm = pd.DataFrame(df_test_predict['result'].value_counts(normalize=True)*100)\n",
    "\n",
    "# print results\n",
    "print(f\"hate prediction: {norm.loc['hate','result']}\")\n",
    "print(f\"offensive prediction: {norm.loc['offensive','result']}\")\n",
    "print(f\"neutral prediction: {norm.loc['neutral','result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_compare = pd.merge(test,df_test_predict,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nielson Media Research final numbers on ACCEPT...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you to all of the television viewers tha...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you imagine if I had the small crowds that...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NATO commander agrees members should pay up vi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, NATO's top commander just announced that ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase     result\n",
       "0  Nielson Media Research final numbers on ACCEPT...    neutral\n",
       "1  Thank you to all of the television viewers tha...  offensive\n",
       "2  Can you imagine if I had the small crowds that...    neutral\n",
       "3  NATO commander agrees members should pay up vi...    neutral\n",
       "4  Wow, NATO's top commander just announced that ...    neutral"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "df_test_compare.to_csv(\"../LSTM/trump_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
