{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import seaborn as sns\n",
    "import shap # for SHAP value\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Use Keras Tensorflow deeplearning library\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import eli5 # for permutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from pdpbox import pdp, info_plots # flor partial plots \n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from joblib import dump, load\n",
    "seed_value = 123\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset: Need only class as \"Sentiment\" and text as 'Phrase'\n",
    "train = train.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "train = train.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset: Need only text as 'Phrase'\n",
    "test = test.rename(columns={'Text':'Phrase'})\n",
    "test = test.drop(['Date', 'Favorites', 'Retweets', 'Tweet ID'],axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to Save the split texts before cleaning and tokenizing\n",
    "# # Collect dependent values and convert to ONE-HOT encoding\n",
    "# # Output using to_categorical\n",
    "# target_t = train.Sentiment.values\n",
    "# y_target_t = to_categorical(target_t)\n",
    "\n",
    "# # Save the texts before tokenizing (must use the same random seed)\n",
    "# X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(train,y_target_t,\n",
    "#                                                           test_size=0.2,\n",
    "#                                                           random_state=seed_value,\n",
    "#                                                           stratify=y_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    tweets = []\n",
    "    \n",
    "#     for sent in tqdm(df['Phrase']):\n",
    "    for sent in df['Phrase']:\n",
    "        # remove non-alphabetic characters\n",
    "        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(sent))\n",
    "        \n",
    "        #remove html content\n",
    "        tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        words = word_tokenize(tweet_text.lower())\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        tweets.append(lemma_words)\n",
    "        \n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned tweets for both train and test set retrieved\n",
    "\n",
    "train_sentences = clean_sentences(train)\n",
    "\n",
    "test_sentences = clean_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target = train.Sentiment.values\n",
    "y_target = to_categorical(target)\n",
    "num_classes = y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sentences,\n",
    "                                                  y_target,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=seed_value,\n",
    "                                                  stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the no of unique words and max length of a tweet available in the list of cleaned tweets\n",
    "# It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "# Build an unordered collection of unique elements.\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "# for sent in tqdm(X_train):\n",
    "for sent in X_train:\n",
    "\n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max=len(sent)\n",
    "\n",
    "# length of the list of unique_words gives the number of unique words\n",
    "\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual tokenizer of keras and convert to sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# texts_to_sequences\n",
    "# ARGUMENTS: list of texts to turn to sequences\n",
    "# RETURN: list of sequences (one per text input)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is done to equalize the lengths of all input tweets.\n",
    "# LTSM networks need all inputs to be same length.\n",
    "# Therefore, tweets lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "# Also, you always have to give a three-dimensional array as an input to your LSTM network\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting deep learning neural network models\n",
    "# This is a method that allows you to specify an arbitrary large number of training epochs.\n",
    "# This stops training once the model performance stops improving on a hold out validation dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor = 'val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with just acc as metric\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu')) #try elu\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with no metrics\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model_1.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model_1.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model_1.add(Dense(100,activation='relu')) #try elu\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes,activation='softmax'))\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas df for y_train\n",
    "# Describe classifier hate (0), offensive (1), neither or neutral (2)\n",
    "\n",
    "y_df = pd.DataFrame(y_train)\n",
    "y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of number of data points in each category\n",
    "hate_ct = y_train.sum(axis = 0)[0]\n",
    "offensive_ct = y_train.sum(axis = 0)[1]\n",
    "neither_ct = y_train.sum(axis = 0)[2]\n",
    "total_ct = y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the inverse ratio of each category to use for the weights of the model\n",
    "inv_ratio_hate = 1 - (hate_ct / total_ct)\n",
    "inv_ratio_hurtful = 1 - (offensive_ct / total_ct)\n",
    "inv_ratio_neither = 1 - (neither_ct / total_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the no. of records for each classifier\n",
    "hate_num = y_df[0].sum()\n",
    "offensive_num = y_df[1].sum()\n",
    "neutral_num = y_df[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inverse ratio so that hate and neutral has a higher weight\n",
    "print(f' K Hate:{1 - hate_num / len(y_df)}')\n",
    "print(f' S Hate:{1 - (hate_num / len(y_df))}')\n",
    "\n",
    "print(f'K neutral: {1 - neutral_num / len(y_df)}')\n",
    "print(f'S neutral: {1 - (neutral_num / len(y_df))}')\n",
    "\n",
    "# Inverse ratio will give offensive a lower weight\n",
    "print(f'K Offensive: {1 - offensive_num / len(y_df)}')\n",
    "print(f'S Offensive: {1 - (offensive_num / len(y_df))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model adjusting for epochs, batch, and weight\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: inv_ratio_hate,\n",
    "                  1: inv_ratio_hurtful,\n",
    "                  2: inv_ratio_neither} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: inv_ratio_hate,\n",
    "                  1: inv_ratio_hurtful,\n",
    "                  2: inv_ratio_neither} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation sentiment!\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "# print(y_pred)\n",
    "\n",
    "y_pred_1 = model_1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and Sklearn read arrays differently\n",
    "# Create function to convert keras array to show only one highest sentiment result per list\n",
    "def keras_output_sklearn(y):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for element in y:\n",
    "        result.append(np.argmax(element))\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Report for model with 'acc' metric\n",
    "# report = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred))\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report for model with no metrics \n",
    "report_1 = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1))\n",
    "print(report_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model with 'acc' metric\n",
    "model.save('../models/model_acc.h5')\n",
    "\n",
    "# Saving model with no metrics\n",
    "model_1.save('../models/model_no.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
