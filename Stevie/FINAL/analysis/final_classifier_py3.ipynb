{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to generate the finalized version of the classifier, to simply feature transformation into the final form, and to test that the results are the same\n",
    "\n",
    "Most of the code comes from operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import *\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "#Loading raw data\n",
    "df = pd.read_csv(\"../Data/labeled_data.csv\")\n",
    "tweets = df.tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b b': 0,\n",
       " 'b b b': 1,\n",
       " 'b b c': 2,\n",
       " 'b b e': 3,\n",
       " 'b b f': 4,\n",
       " 'b b g': 5,\n",
       " 'b b h': 6,\n",
       " 'b b k': 7,\n",
       " 'b b l': 8,\n",
       " 'b b n': 9,\n",
       " 'b b p': 10,\n",
       " 'b b q': 11,\n",
       " 'b b r': 12,\n",
       " 'b b u': 13,\n",
       " 'b b w': 14,\n",
       " 'b c': 15,\n",
       " 'b c b': 16,\n",
       " 'b c c': 17,\n",
       " 'b c e': 18,\n",
       " 'b c f': 19,\n",
       " 'b c g': 20,\n",
       " 'b c h': 21,\n",
       " 'b c k': 22,\n",
       " 'b c l': 23,\n",
       " 'b c n': 24,\n",
       " 'b c p': 25,\n",
       " 'b c r': 26,\n",
       " 'b c u': 27,\n",
       " 'b c v': 28,\n",
       " 'b c w': 29,\n",
       " 'b e': 30,\n",
       " 'b e b': 31,\n",
       " 'b e c': 32,\n",
       " 'b e e': 33,\n",
       " 'b e f': 34,\n",
       " 'b e g': 35,\n",
       " 'b e h': 36,\n",
       " 'b e j': 37,\n",
       " 'b e k': 38,\n",
       " 'b e l': 39,\n",
       " 'b e n': 40,\n",
       " 'b e p': 41,\n",
       " 'b e q': 42,\n",
       " 'b e r': 43,\n",
       " 'b e u': 44,\n",
       " 'b e v': 45,\n",
       " 'b e w': 46,\n",
       " 'b e x': 47,\n",
       " 'b e z': 48,\n",
       " 'b f': 49,\n",
       " 'b f b': 50,\n",
       " 'b f c': 51,\n",
       " 'b f e': 52,\n",
       " 'b f f': 53,\n",
       " 'b f g': 54,\n",
       " 'b f h': 55,\n",
       " 'b f l': 56,\n",
       " 'b f n': 57,\n",
       " 'b f r': 58,\n",
       " 'b f u': 59,\n",
       " 'b f w': 60,\n",
       " 'b g': 61,\n",
       " 'b g b': 62,\n",
       " 'b g c': 63,\n",
       " 'b g e': 64,\n",
       " 'b g f': 65,\n",
       " 'b g g': 66,\n",
       " 'b g h': 67,\n",
       " 'b g l': 68,\n",
       " 'b g n': 69,\n",
       " 'b g p': 70,\n",
       " 'b g r': 71,\n",
       " 'b g u': 72,\n",
       " 'b g v': 73,\n",
       " 'b g w': 74,\n",
       " 'b h': 75,\n",
       " 'b h b': 76,\n",
       " 'b h c': 77,\n",
       " 'b h e': 78,\n",
       " 'b h f': 79,\n",
       " 'b h g': 80,\n",
       " 'b h h': 81,\n",
       " 'b h k': 82,\n",
       " 'b h l': 83,\n",
       " 'b h n': 84,\n",
       " 'b h p': 85,\n",
       " 'b h r': 86,\n",
       " 'b h u': 87,\n",
       " 'b h v': 88,\n",
       " 'b h w': 89,\n",
       " 'b j': 90,\n",
       " 'b j e': 91,\n",
       " 'b j h': 92,\n",
       " 'b j n': 93,\n",
       " 'b j u': 94,\n",
       " 'b k': 95,\n",
       " 'b k b': 96,\n",
       " 'b k c': 97,\n",
       " 'b k e': 98,\n",
       " 'b k f': 99,\n",
       " 'b k g': 100,\n",
       " 'b k h': 101,\n",
       " 'b k l': 102,\n",
       " 'b k n': 103,\n",
       " 'b k r': 104,\n",
       " 'b k u': 105,\n",
       " 'b k w': 106,\n",
       " 'b l': 107,\n",
       " 'b l b': 108,\n",
       " 'b l c': 109,\n",
       " 'b l e': 110,\n",
       " 'b l f': 111,\n",
       " 'b l g': 112,\n",
       " 'b l h': 113,\n",
       " 'b l j': 114,\n",
       " 'b l k': 115,\n",
       " 'b l l': 116,\n",
       " 'b l n': 117,\n",
       " 'b l p': 118,\n",
       " 'b l r': 119,\n",
       " 'b l u': 120,\n",
       " 'b l v': 121,\n",
       " 'b l w': 122,\n",
       " 'b l x': 123,\n",
       " 'b n': 124,\n",
       " 'b n b': 125,\n",
       " 'b n c': 126,\n",
       " 'b n e': 127,\n",
       " 'b n f': 128,\n",
       " 'b n g': 129,\n",
       " 'b n h': 130,\n",
       " 'b n j': 131,\n",
       " 'b n k': 132,\n",
       " 'b n l': 133,\n",
       " 'b n n': 134,\n",
       " 'b n p': 135,\n",
       " 'b n r': 136,\n",
       " 'b n u': 137,\n",
       " 'b n w': 138,\n",
       " 'b n x': 139,\n",
       " 'b p': 140,\n",
       " 'b p c': 141,\n",
       " 'b p e': 142,\n",
       " 'b p h': 143,\n",
       " 'b p l': 144,\n",
       " 'b p n': 145,\n",
       " 'b p p': 146,\n",
       " 'b p r': 147,\n",
       " 'b p u': 148,\n",
       " 'b q': 149,\n",
       " 'b q u': 150,\n",
       " 'b r': 151,\n",
       " 'b r b': 152,\n",
       " 'b r c': 153,\n",
       " 'b r e': 154,\n",
       " 'b r f': 155,\n",
       " 'b r g': 156,\n",
       " 'b r h': 157,\n",
       " 'b r j': 158,\n",
       " 'b r k': 159,\n",
       " 'b r l': 160,\n",
       " 'b r n': 161,\n",
       " 'b r p': 162,\n",
       " 'b r r': 163,\n",
       " 'b r u': 164,\n",
       " 'b r v': 165,\n",
       " 'b r w': 166,\n",
       " 'b r z': 167,\n",
       " 'b u': 168,\n",
       " 'b u b': 169,\n",
       " 'b u c': 170,\n",
       " 'b u e': 171,\n",
       " 'b u f': 172,\n",
       " 'b u g': 173,\n",
       " 'b u h': 174,\n",
       " 'b u j': 175,\n",
       " 'b u k': 176,\n",
       " 'b u l': 177,\n",
       " 'b u n': 178,\n",
       " 'b u p': 179,\n",
       " 'b u q': 180,\n",
       " 'b u r': 181,\n",
       " 'b u u': 182,\n",
       " 'b u v': 183,\n",
       " 'b u w': 184,\n",
       " 'b u z': 185,\n",
       " 'b v': 186,\n",
       " 'b v b': 187,\n",
       " 'b v e': 188,\n",
       " 'b v u': 189,\n",
       " 'b w': 190,\n",
       " 'b w b': 191,\n",
       " 'b w c': 192,\n",
       " 'b w e': 193,\n",
       " 'b w f': 194,\n",
       " 'b w g': 195,\n",
       " 'b w h': 196,\n",
       " 'b w l': 197,\n",
       " 'b w n': 198,\n",
       " 'b w p': 199,\n",
       " 'b w r': 200,\n",
       " 'b w u': 201,\n",
       " 'b w w': 202,\n",
       " 'b x': 203,\n",
       " 'b x e': 204,\n",
       " 'b x f': 205,\n",
       " 'b x h': 206,\n",
       " 'b x n': 207,\n",
       " 'b z': 208,\n",
       " 'b z e': 209,\n",
       " 'b z z': 210,\n",
       " 'c b': 211,\n",
       " 'c b b': 212,\n",
       " 'c b c': 213,\n",
       " 'c b e': 214,\n",
       " 'c b h': 215,\n",
       " 'c b l': 216,\n",
       " 'c b n': 217,\n",
       " 'c b r': 218,\n",
       " 'c b u': 219,\n",
       " 'c b w': 220,\n",
       " 'c c': 221,\n",
       " 'c c b': 222,\n",
       " 'c c c': 223,\n",
       " 'c c e': 224,\n",
       " 'c c f': 225,\n",
       " 'c c g': 226,\n",
       " 'c c h': 227,\n",
       " 'c c k': 228,\n",
       " 'c c l': 229,\n",
       " 'c c n': 230,\n",
       " 'c c p': 231,\n",
       " 'c c r': 232,\n",
       " 'c c u': 233,\n",
       " 'c c w': 234,\n",
       " 'c e': 235,\n",
       " 'c e b': 236,\n",
       " 'c e c': 237,\n",
       " 'c e e': 238,\n",
       " 'c e f': 239,\n",
       " 'c e g': 240,\n",
       " 'c e h': 241,\n",
       " 'c e j': 242,\n",
       " 'c e k': 243,\n",
       " 'c e l': 244,\n",
       " 'c e n': 245,\n",
       " 'c e p': 246,\n",
       " 'c e r': 247,\n",
       " 'c e u': 248,\n",
       " 'c e v': 249,\n",
       " 'c e w': 250,\n",
       " 'c e x': 251,\n",
       " 'c f': 252,\n",
       " 'c f c': 253,\n",
       " 'c f e': 254,\n",
       " 'c f f': 255,\n",
       " 'c f g': 256,\n",
       " 'c f h': 257,\n",
       " 'c f l': 258,\n",
       " 'c f n': 259,\n",
       " 'c f r': 260,\n",
       " 'c f u': 261,\n",
       " 'c g': 262,\n",
       " 'c g b': 263,\n",
       " 'c g e': 264,\n",
       " 'c g h': 265,\n",
       " 'c g n': 266,\n",
       " 'c g p': 267,\n",
       " 'c g r': 268,\n",
       " 'c g u': 269,\n",
       " 'c h b': 270,\n",
       " 'c h c': 271,\n",
       " 'c h e': 272,\n",
       " 'c h f': 273,\n",
       " 'c h g': 274,\n",
       " 'c h h': 275,\n",
       " 'c h j': 276,\n",
       " 'c h k': 277,\n",
       " 'c h l': 278,\n",
       " 'c h n': 279,\n",
       " 'c h p': 280,\n",
       " 'c h q': 281,\n",
       " 'c h r': 282,\n",
       " 'c h u': 283,\n",
       " 'c h v': 284,\n",
       " 'c h w': 285,\n",
       " 'c h x': 286,\n",
       " 'c h z': 287,\n",
       " 'c j': 288,\n",
       " 'c j c': 289,\n",
       " 'c j h': 290,\n",
       " 'c j u': 291,\n",
       " 'c k': 292,\n",
       " 'c k b': 293,\n",
       " 'c k c': 294,\n",
       " 'c k e': 295,\n",
       " 'c k f': 296,\n",
       " 'c k g': 297,\n",
       " 'c k h': 298,\n",
       " 'c k j': 299,\n",
       " 'c k k': 300,\n",
       " 'c k l': 301,\n",
       " 'c k n': 302,\n",
       " 'c k p': 303,\n",
       " 'c k q': 304,\n",
       " 'c k r': 305,\n",
       " 'c k u': 306,\n",
       " 'c k v': 307,\n",
       " 'c k w': 308,\n",
       " 'c k z': 309,\n",
       " 'c l': 310,\n",
       " 'c l b': 311,\n",
       " 'c l c': 312,\n",
       " 'c l e': 313,\n",
       " 'c l f': 314,\n",
       " 'c l g': 315,\n",
       " 'c l h': 316,\n",
       " 'c l j': 317,\n",
       " 'c l k': 318,\n",
       " 'c l l': 319,\n",
       " 'c l n': 320,\n",
       " 'c l p': 321,\n",
       " 'c l q': 322,\n",
       " 'c l r': 323,\n",
       " 'c l u': 324,\n",
       " 'c l v': 325,\n",
       " 'c l w': 326,\n",
       " 'c l z': 327,\n",
       " 'c n': 328,\n",
       " 'c n b': 329,\n",
       " 'c n c': 330,\n",
       " 'c n e': 331,\n",
       " 'c n f': 332,\n",
       " 'c n g': 333,\n",
       " 'c n h': 334,\n",
       " 'c n j': 335,\n",
       " 'c n k': 336,\n",
       " 'c n l': 337,\n",
       " 'c n n': 338,\n",
       " 'c n p': 339,\n",
       " 'c n q': 340,\n",
       " 'c n r': 341,\n",
       " 'c n u': 342,\n",
       " 'c n v': 343,\n",
       " 'c n w': 344,\n",
       " 'c p': 345,\n",
       " 'c p b': 346,\n",
       " 'c p c': 347,\n",
       " 'c p e': 348,\n",
       " 'c p f': 349,\n",
       " 'c p g': 350,\n",
       " 'c p h': 351,\n",
       " 'c p j': 352,\n",
       " 'c p l': 353,\n",
       " 'c p n': 354,\n",
       " 'c p p': 355,\n",
       " 'c p r': 356,\n",
       " 'c p u': 357,\n",
       " 'c p w': 358,\n",
       " 'c q': 359,\n",
       " 'c q u': 360,\n",
       " 'c r': 361,\n",
       " 'c r b': 362,\n",
       " 'c r c': 363,\n",
       " 'c r e': 364,\n",
       " 'c r f': 365,\n",
       " 'c r g': 366,\n",
       " 'c r h': 367,\n",
       " 'c r j': 368,\n",
       " 'c r k': 369,\n",
       " 'c r l': 370,\n",
       " 'c r n': 371,\n",
       " 'c r p': 372,\n",
       " 'c r r': 373,\n",
       " 'c r u': 374,\n",
       " 'c r v': 375,\n",
       " 'c r w': 376,\n",
       " 'c r z': 377,\n",
       " 'c u': 378,\n",
       " 'c u b': 379,\n",
       " 'c u c': 380,\n",
       " 'c u e': 381,\n",
       " 'c u f': 382,\n",
       " 'c u g': 383,\n",
       " 'c u h': 384,\n",
       " 'c u k': 385,\n",
       " 'c u l': 386,\n",
       " 'c u n': 387,\n",
       " 'c u p': 388,\n",
       " 'c u r': 389,\n",
       " 'c u u': 390,\n",
       " 'c u w': 391,\n",
       " 'c u z': 392,\n",
       " 'c v': 393,\n",
       " 'c v b': 394,\n",
       " 'c v c': 395,\n",
       " 'c v e': 396,\n",
       " 'c v l': 397,\n",
       " 'c w': 398,\n",
       " 'c w b': 399,\n",
       " 'c w c': 400,\n",
       " 'c w e': 401,\n",
       " 'c w g': 402,\n",
       " 'c w h': 403,\n",
       " 'c w l': 404,\n",
       " 'c w n': 405,\n",
       " 'c w p': 406,\n",
       " 'c w r': 407,\n",
       " 'c w u': 408,\n",
       " 'c x': 409,\n",
       " 'c z': 410,\n",
       " 'c z e': 411,\n",
       " 'e b': 412,\n",
       " 'e b b': 413,\n",
       " 'e b c': 414,\n",
       " 'e b e': 415,\n",
       " 'e b f': 416,\n",
       " 'e b g': 417,\n",
       " 'e b h': 418,\n",
       " 'e b j': 419,\n",
       " 'e b k': 420,\n",
       " 'e b l': 421,\n",
       " 'e b n': 422,\n",
       " 'e b p': 423,\n",
       " 'e b r': 424,\n",
       " 'e b u': 425,\n",
       " 'e b v': 426,\n",
       " 'e b w': 427,\n",
       " 'e b x': 428,\n",
       " 'e b z': 429,\n",
       " 'e c': 430,\n",
       " 'e c b': 431,\n",
       " 'e c c': 432,\n",
       " 'e c e': 433,\n",
       " 'e c f': 434,\n",
       " 'e c g': 435,\n",
       " 'e c h': 436,\n",
       " 'e c k': 437,\n",
       " 'e c l': 438,\n",
       " 'e c n': 439,\n",
       " 'e c p': 440,\n",
       " 'e c r': 441,\n",
       " 'e c u': 442,\n",
       " 'e c v': 443,\n",
       " 'e c w': 444,\n",
       " 'e e': 445,\n",
       " 'e e b': 446,\n",
       " 'e e c': 447,\n",
       " 'e e e': 448,\n",
       " 'e e f': 449,\n",
       " 'e e g': 450,\n",
       " 'e e h': 451,\n",
       " 'e e j': 452,\n",
       " 'e e k': 453,\n",
       " 'e e l': 454,\n",
       " 'e e n': 455,\n",
       " 'e e p': 456,\n",
       " 'e e q': 457,\n",
       " 'e e r': 458,\n",
       " 'e e u': 459,\n",
       " 'e e v': 460,\n",
       " 'e e w': 461,\n",
       " 'e e x': 462,\n",
       " 'e e z': 463,\n",
       " 'e f': 464,\n",
       " 'e f b': 465,\n",
       " 'e f c': 466,\n",
       " 'e f e': 467,\n",
       " 'e f f': 468,\n",
       " 'e f g': 469,\n",
       " 'e f h': 470,\n",
       " 'e f j': 471,\n",
       " 'e f k': 472,\n",
       " 'e f l': 473,\n",
       " 'e f n': 474,\n",
       " 'e f p': 475,\n",
       " 'e f r': 476,\n",
       " 'e f u': 477,\n",
       " 'e f v': 478,\n",
       " 'e f w': 479,\n",
       " 'e f x': 480,\n",
       " 'e g': 481,\n",
       " 'e g b': 482,\n",
       " 'e g c': 483,\n",
       " 'e g e': 484,\n",
       " 'e g f': 485,\n",
       " 'e g g': 486,\n",
       " 'e g h': 487,\n",
       " 'e g j': 488,\n",
       " 'e g k': 489,\n",
       " 'e g l': 490,\n",
       " 'e g n': 491,\n",
       " 'e g p': 492,\n",
       " 'e g q': 493,\n",
       " 'e g r': 494,\n",
       " 'e g u': 495,\n",
       " 'e g v': 496,\n",
       " 'e g w': 497,\n",
       " 'e g z': 498,\n",
       " 'e h': 499,\n",
       " 'e h b': 500,\n",
       " 'e h c': 501,\n",
       " 'e h e': 502,\n",
       " 'e h f': 503,\n",
       " 'e h g': 504,\n",
       " 'e h h': 505,\n",
       " 'e h j': 506,\n",
       " 'e h k': 507,\n",
       " 'e h l': 508,\n",
       " 'e h n': 509,\n",
       " 'e h p': 510,\n",
       " 'e h q': 511,\n",
       " 'e h r': 512,\n",
       " 'e h u': 513,\n",
       " 'e h v': 514,\n",
       " 'e h w': 515,\n",
       " 'e j': 516,\n",
       " 'e j b': 517,\n",
       " 'e j c': 518,\n",
       " 'e j e': 519,\n",
       " 'e j f': 520,\n",
       " 'e j g': 521,\n",
       " 'e j h': 522,\n",
       " 'e j j': 523,\n",
       " 'e j k': 524,\n",
       " 'e j l': 525,\n",
       " 'e j n': 526,\n",
       " 'e j p': 527,\n",
       " 'e j r': 528,\n",
       " 'e j u': 529,\n",
       " 'e j w': 530,\n",
       " 'e k': 531,\n",
       " 'e k b': 532,\n",
       " 'e k c': 533,\n",
       " 'e k e': 534,\n",
       " 'e k f': 535,\n",
       " 'e k g': 536,\n",
       " 'e k h': 537,\n",
       " 'e k j': 538,\n",
       " 'e k k': 539,\n",
       " 'e k l': 540,\n",
       " 'e k n': 541,\n",
       " 'e k p': 542,\n",
       " 'e k r': 543,\n",
       " 'e k u': 544,\n",
       " 'e k v': 545,\n",
       " 'e k w': 546,\n",
       " 'e l': 547,\n",
       " 'e l b': 548,\n",
       " 'e l c': 549,\n",
       " 'e l e': 550,\n",
       " 'e l f': 551,\n",
       " 'e l g': 552,\n",
       " 'e l h': 553,\n",
       " 'e l j': 554,\n",
       " 'e l k': 555,\n",
       " 'e l l': 556,\n",
       " 'e l n': 557,\n",
       " 'e l p': 558,\n",
       " 'e l q': 559,\n",
       " 'e l r': 560,\n",
       " 'e l u': 561,\n",
       " 'e l v': 562,\n",
       " 'e l w': 563,\n",
       " 'e l x': 564,\n",
       " 'e l z': 565,\n",
       " 'e n': 566,\n",
       " 'e n b': 567,\n",
       " 'e n c': 568,\n",
       " 'e n e': 569,\n",
       " 'e n f': 570,\n",
       " 'e n g': 571,\n",
       " 'e n h': 572,\n",
       " 'e n j': 573,\n",
       " 'e n k': 574,\n",
       " 'e n l': 575,\n",
       " 'e n n': 576,\n",
       " 'e n p': 577,\n",
       " 'e n q': 578,\n",
       " 'e n r': 579,\n",
       " 'e n u': 580,\n",
       " 'e n v': 581,\n",
       " 'e n w': 582,\n",
       " 'e n x': 583,\n",
       " 'e n z': 584,\n",
       " 'e p': 585,\n",
       " 'e p b': 586,\n",
       " 'e p c': 587,\n",
       " 'e p e': 588,\n",
       " 'e p f': 589,\n",
       " 'e p g': 590,\n",
       " 'e p h': 591,\n",
       " 'e p j': 592,\n",
       " 'e p k': 593,\n",
       " 'e p l': 594,\n",
       " 'e p n': 595,\n",
       " 'e p p': 596,\n",
       " 'e p r': 597,\n",
       " 'e p u': 598,\n",
       " 'e p v': 599,\n",
       " 'e p w': 600,\n",
       " 'e p z': 601,\n",
       " 'e q': 602,\n",
       " 'e q u': 603,\n",
       " 'e r': 604,\n",
       " 'e r b': 605,\n",
       " 'e r c': 606,\n",
       " 'e r e': 607,\n",
       " 'e r f': 608,\n",
       " 'e r g': 609,\n",
       " 'e r h': 610,\n",
       " 'e r j': 611,\n",
       " 'e r k': 612,\n",
       " 'e r l': 613,\n",
       " 'e r n': 614,\n",
       " 'e r p': 615,\n",
       " 'e r q': 616,\n",
       " 'e r r': 617,\n",
       " 'e r u': 618,\n",
       " 'e r v': 619,\n",
       " 'e r w': 620,\n",
       " 'e r x': 621,\n",
       " 'e r z': 622,\n",
       " 'e u': 623,\n",
       " 'e u b': 624,\n",
       " 'e u c': 625,\n",
       " 'e u e': 626,\n",
       " 'e u f': 627,\n",
       " 'e u g': 628,\n",
       " 'e u h': 629,\n",
       " 'e u j': 630,\n",
       " 'e u k': 631,\n",
       " 'e u l': 632,\n",
       " 'e u n': 633,\n",
       " 'e u p': 634,\n",
       " 'e u q': 635,\n",
       " 'e u r': 636,\n",
       " 'e u u': 637,\n",
       " 'e u v': 638,\n",
       " 'e u w': 639,\n",
       " 'e u x': 640,\n",
       " 'e u z': 641,\n",
       " 'e v': 642,\n",
       " 'e v b': 643,\n",
       " 'e v c': 644,\n",
       " 'e v e': 645,\n",
       " 'e v g': 646,\n",
       " 'e v h': 647,\n",
       " 'e v k': 648,\n",
       " 'e v l': 649,\n",
       " 'e v n': 650,\n",
       " 'e v p': 651,\n",
       " 'e v r': 652,\n",
       " 'e v u': 653,\n",
       " 'e v v': 654,\n",
       " 'e v w': 655,\n",
       " 'e w': 656,\n",
       " 'e w b': 657,\n",
       " 'e w c': 658,\n",
       " 'e w e': 659,\n",
       " 'e w f': 660,\n",
       " 'e w g': 661,\n",
       " 'e w h': 662,\n",
       " 'e w j': 663,\n",
       " 'e w k': 664,\n",
       " 'e w l': 665,\n",
       " 'e w n': 666,\n",
       " 'e w p': 667,\n",
       " 'e w r': 668,\n",
       " 'e w u': 669,\n",
       " 'e w v': 670,\n",
       " 'e w w': 671,\n",
       " 'e x': 672,\n",
       " 'e x b': 673,\n",
       " 'e x c': 674,\n",
       " 'e x e': 675,\n",
       " 'e x f': 676,\n",
       " 'e x g': 677,\n",
       " 'e x h': 678,\n",
       " 'e x j': 679,\n",
       " 'e x k': 680,\n",
       " 'e x l': 681,\n",
       " 'e x n': 682,\n",
       " 'e x p': 683,\n",
       " 'e x r': 684,\n",
       " 'e x u': 685,\n",
       " 'e x v': 686,\n",
       " 'e x w': 687,\n",
       " 'e x x': 688,\n",
       " 'e z': 689,\n",
       " 'e z b': 690,\n",
       " 'e z c': 691,\n",
       " 'e z e': 692,\n",
       " 'e z f': 693,\n",
       " 'e z g': 694,\n",
       " 'e z h': 695,\n",
       " 'e z n': 696,\n",
       " 'e z p': 697,\n",
       " 'e z r': 698,\n",
       " 'e z u': 699,\n",
       " 'e z z': 700,\n",
       " 'f b': 701,\n",
       " 'f b b': 702,\n",
       " 'f b c': 703,\n",
       " 'f b e': 704,\n",
       " 'f b f': 705,\n",
       " 'f b g': 706,\n",
       " 'f b h': 707,\n",
       " 'f b l': 708,\n",
       " 'f b n': 709,\n",
       " 'f b r': 710,\n",
       " 'f b u': 711,\n",
       " 'f b w': 712,\n",
       " 'f c': 713,\n",
       " 'f c b': 714,\n",
       " 'f c c': 715,\n",
       " 'f c e': 716,\n",
       " 'f c f': 717,\n",
       " 'f c h': 718,\n",
       " 'f c k': 719,\n",
       " 'f c l': 720,\n",
       " 'f c n': 721,\n",
       " 'f c p': 722,\n",
       " 'f c r': 723,\n",
       " 'f c u': 724,\n",
       " 'f c w': 725,\n",
       " 'f e': 726,\n",
       " 'f e b': 727,\n",
       " 'f e c': 728,\n",
       " 'f e e': 729,\n",
       " 'f e f': 730,\n",
       " 'f e g': 731,\n",
       " 'f e h': 732,\n",
       " 'f e j': 733,\n",
       " 'f e k': 734,\n",
       " 'f e l': 735,\n",
       " 'f e n': 736,\n",
       " 'f e p': 737,\n",
       " 'f e r': 738,\n",
       " 'f e u': 739,\n",
       " 'f e v': 740,\n",
       " 'f e w': 741,\n",
       " 'f e x': 742,\n",
       " 'f f': 743,\n",
       " 'f f b': 744,\n",
       " 'f f c': 745,\n",
       " 'f f e': 746,\n",
       " 'f f f': 747,\n",
       " 'f f g': 748,\n",
       " 'f f h': 749,\n",
       " 'f f j': 750,\n",
       " 'f f k': 751,\n",
       " 'f f l': 752,\n",
       " 'f f n': 753,\n",
       " 'f f p': 754,\n",
       " 'f f r': 755,\n",
       " 'f f u': 756,\n",
       " 'f f v': 757,\n",
       " 'f f w': 758,\n",
       " 'f g': 759,\n",
       " 'f g b': 760,\n",
       " 'f g c': 761,\n",
       " 'f g e': 762,\n",
       " 'f g f': 763,\n",
       " 'f g g': 764,\n",
       " 'f g h': 765,\n",
       " 'f g l': 766,\n",
       " 'f g n': 767,\n",
       " 'f g p': 768,\n",
       " 'f g r': 769,\n",
       " 'f g u': 770,\n",
       " 'f g w': 771,\n",
       " 'f h': 772,\n",
       " 'f h b': 773,\n",
       " 'f h c': 774,\n",
       " 'f h e': 775,\n",
       " 'f h f': 776,\n",
       " 'f h g': 777,\n",
       " 'f h h': 778,\n",
       " 'f h k': 779,\n",
       " 'f h l': 780,\n",
       " 'f h n': 781,\n",
       " 'f h p': 782,\n",
       " 'f h r': 783,\n",
       " 'f h u': 784,\n",
       " 'f h v': 785,\n",
       " 'f h w': 786,\n",
       " 'f j': 787,\n",
       " 'f j e': 788,\n",
       " 'f j k': 789,\n",
       " 'f j u': 790,\n",
       " 'f k': 791,\n",
       " 'f k e': 792,\n",
       " 'f k n': 793,\n",
       " 'f l': 794,\n",
       " 'f l b': 795,\n",
       " 'f l c': 796,\n",
       " 'f l e': 797,\n",
       " 'f l f': 798,\n",
       " 'f l g': 799,\n",
       " 'f l h': 800,\n",
       " 'f l k': 801,\n",
       " 'f l l': 802,\n",
       " 'f l n': 803,\n",
       " 'f l p': 804,\n",
       " 'f l r': 805,\n",
       " 'f l u': 806,\n",
       " 'f l v': 807,\n",
       " 'f l w': 808,\n",
       " 'f l x': 809,\n",
       " 'f n': 810,\n",
       " 'f n b': 811,\n",
       " 'f n c': 812,\n",
       " 'f n e': 813,\n",
       " 'f n f': 814,\n",
       " 'f n g': 815,\n",
       " 'f n h': 816,\n",
       " 'f n j': 817,\n",
       " 'f n k': 818,\n",
       " 'f n l': 819,\n",
       " 'f n n': 820,\n",
       " 'f n p': 821,\n",
       " 'f n r': 822,\n",
       " 'f n u': 823,\n",
       " 'f n v': 824,\n",
       " 'f n w': 825,\n",
       " 'f p': 826,\n",
       " 'f p b': 827,\n",
       " 'f p c': 828,\n",
       " 'f p e': 829,\n",
       " 'f p h': 830,\n",
       " 'f p l': 831,\n",
       " 'f p n': 832,\n",
       " 'f p p': 833,\n",
       " 'f p r': 834,\n",
       " 'f p u': 835,\n",
       " 'f p w': 836,\n",
       " 'f q': 837,\n",
       " 'f q u': 838,\n",
       " 'f r': 839,\n",
       " 'f r b': 840,\n",
       " 'f r c': 841,\n",
       " 'f r e': 842,\n",
       " 'f r f': 843,\n",
       " 'f r g': 844,\n",
       " 'f r h': 845,\n",
       " 'f r j': 846,\n",
       " 'f r k': 847,\n",
       " 'f r l': 848,\n",
       " 'f r n': 849,\n",
       " 'f r p': 850,\n",
       " 'f r q': 851,\n",
       " 'f r r': 852,\n",
       " 'f r u': 853,\n",
       " 'f r v': 854,\n",
       " 'f r w': 855,\n",
       " 'f r z': 856,\n",
       " 'f u': 857,\n",
       " 'f u b': 858,\n",
       " 'f u c': 859,\n",
       " 'f u e': 860,\n",
       " 'f u f': 861,\n",
       " 'f u g': 862,\n",
       " 'f u h': 863,\n",
       " 'f u j': 864,\n",
       " 'f u k': 865,\n",
       " 'f u l': 866,\n",
       " 'f u n': 867,\n",
       " 'f u p': 868,\n",
       " 'f u q': 869,\n",
       " 'f u r': 870,\n",
       " 'f u u': 871,\n",
       " 'f u v': 872,\n",
       " 'f u w': 873,\n",
       " 'f u x': 874,\n",
       " 'f u z': 875,\n",
       " 'f v': 876,\n",
       " 'f v c': 877,\n",
       " 'f v e': 878,\n",
       " 'f v l': 879,\n",
       " 'f v n': 880,\n",
       " 'f v r': 881,\n",
       " 'f w': 882,\n",
       " 'f w b': 883,\n",
       " 'f w c': 884,\n",
       " 'f w e': 885,\n",
       " 'f w f': 886,\n",
       " 'f w g': 887,\n",
       " 'f w h': 888,\n",
       " 'f w k': 889,\n",
       " 'f w l': 890,\n",
       " 'f w n': 891,\n",
       " 'f w r': 892,\n",
       " 'f w u': 893,\n",
       " 'f w w': 894,\n",
       " 'f x': 895,\n",
       " 'f x c': 896,\n",
       " 'f x e': 897,\n",
       " 'f x n': 898,\n",
       " 'f z': 899,\n",
       " 'f z z': 900,\n",
       " 'g b': 901,\n",
       " 'g b b': 902,\n",
       " 'g b c': 903,\n",
       " 'g b e': 904,\n",
       " 'g b f': 905,\n",
       " 'g b g': 906,\n",
       " 'g b h': 907,\n",
       " 'g b k': 908,\n",
       " 'g b l': 909,\n",
       " 'g b n': 910,\n",
       " 'g b p': 911,\n",
       " 'g b r': 912,\n",
       " 'g b u': 913,\n",
       " 'g b w': 914,\n",
       " 'g c': 915,\n",
       " 'g c b': 916,\n",
       " 'g c c': 917,\n",
       " 'g c e': 918,\n",
       " 'g c f': 919,\n",
       " 'g c g': 920,\n",
       " 'g c h': 921,\n",
       " 'g c k': 922,\n",
       " 'g c l': 923,\n",
       " 'g c n': 924,\n",
       " 'g c p': 925,\n",
       " 'g c r': 926,\n",
       " 'g c u': 927,\n",
       " 'g c v': 928,\n",
       " 'g e': 929,\n",
       " 'g e b': 930,\n",
       " 'g e c': 931,\n",
       " 'g e e': 932,\n",
       " 'g e f': 933,\n",
       " 'g e g': 934,\n",
       " 'g e h': 935,\n",
       " 'g e j': 936,\n",
       " 'g e k': 937,\n",
       " 'g e l': 938,\n",
       " 'g e n': 939,\n",
       " 'g e p': 940,\n",
       " 'g e r': 941,\n",
       " 'g e u': 942,\n",
       " 'g e v': 943,\n",
       " 'g e w': 944,\n",
       " 'g e x': 945,\n",
       " 'g e z': 946,\n",
       " 'g f': 947,\n",
       " 'g f b': 948,\n",
       " 'g f c': 949,\n",
       " 'g f e': 950,\n",
       " 'g f f': 951,\n",
       " 'g f g': 952,\n",
       " 'g f h': 953,\n",
       " 'g f k': 954,\n",
       " 'g f l': 955,\n",
       " 'g f n': 956,\n",
       " 'g f r': 957,\n",
       " 'g f u': 958,\n",
       " 'g f w': 959,\n",
       " 'g g': 960,\n",
       " 'g g b': 961,\n",
       " 'g g c': 962,\n",
       " 'g g e': 963,\n",
       " 'g g f': 964,\n",
       " 'g g g': 965,\n",
       " 'g g h': 966,\n",
       " 'g g j': 967,\n",
       " 'g g k': 968,\n",
       " 'g g l': 969,\n",
       " 'g g n': 970,\n",
       " 'g g p': 971,\n",
       " 'g g q': 972,\n",
       " 'g g r': 973,\n",
       " 'g g u': 974,\n",
       " 'g g v': 975,\n",
       " 'g g w': 976,\n",
       " 'g g z': 977,\n",
       " 'g h': 978,\n",
       " 'g h b': 979,\n",
       " 'g h c': 980,\n",
       " 'g h e': 981,\n",
       " 'g h f': 982,\n",
       " 'g h g': 983,\n",
       " 'g h h': 984,\n",
       " 'g h j': 985,\n",
       " 'g h k': 986,\n",
       " 'g h l': 987,\n",
       " 'g h n': 988,\n",
       " 'g h p': 989,\n",
       " 'g h q': 990,\n",
       " 'g h r': 991,\n",
       " 'g h u': 992,\n",
       " 'g h v': 993,\n",
       " 'g h w': 994,\n",
       " 'g h z': 995,\n",
       " 'g j': 996,\n",
       " 'g j b': 997,\n",
       " 'g j c': 998,\n",
       " 'g j e': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    +\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 3999)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "This model was found using a GridSearch with 5-fold cross validation. Details are in the notebook operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.44      0.46      1430\n",
      "           1       0.92      0.92      0.92     19190\n",
      "           2       0.75      0.78      0.77      4163\n",
      "\n",
      "    accuracy                           0.87     24783\n",
      "   macro avg       0.72      0.71      0.72     24783\n",
      "weighted avg       0.87      0.87      0.87     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using information from the model to obtain the matrix X_ generically\n",
    "\n",
    "This is the most difficult task: We have to take the inputs tweets and transform them into a format that can be used in the model without going through all the same pre-processing steps as above. This can be done as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = select.get_support(indices=True) #get indices of features\n",
    "final_feature_list = [str(feature_names[i]) for i in final_features] #Get list of names corresponding to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b b',\n",
       " 'b c',\n",
       " 'b c h',\n",
       " 'b e e',\n",
       " 'b e g',\n",
       " 'b e l',\n",
       " 'b e n',\n",
       " 'b e v',\n",
       " 'b h n',\n",
       " 'b l',\n",
       " 'b l c',\n",
       " 'b l l',\n",
       " 'b n',\n",
       " 'b n b',\n",
       " 'b n k',\n",
       " 'b n n',\n",
       " 'b r',\n",
       " 'b r e',\n",
       " 'b r f',\n",
       " 'b r h',\n",
       " 'b r n',\n",
       " 'b r w',\n",
       " 'b u r',\n",
       " 'c e',\n",
       " 'c e h',\n",
       " 'c e n',\n",
       " 'c e r',\n",
       " 'c h e',\n",
       " 'c h g',\n",
       " 'c h n',\n",
       " 'c h r',\n",
       " 'c k c',\n",
       " 'c k e',\n",
       " 'c k f',\n",
       " 'c k h',\n",
       " 'c k n',\n",
       " 'c k u',\n",
       " 'c l',\n",
       " 'c l l',\n",
       " 'c l r',\n",
       " 'c n',\n",
       " 'c n f',\n",
       " 'c n n',\n",
       " 'c n p',\n",
       " 'c n r',\n",
       " 'c p',\n",
       " 'c r',\n",
       " 'c r c',\n",
       " 'c r e',\n",
       " 'c r h',\n",
       " 'c r w',\n",
       " 'c u',\n",
       " 'c u l',\n",
       " 'c u n',\n",
       " 'c u z',\n",
       " 'c w',\n",
       " 'e b',\n",
       " 'e b r',\n",
       " 'e b u',\n",
       " 'e c',\n",
       " 'e c h',\n",
       " 'e c k',\n",
       " 'e e',\n",
       " 'e e b',\n",
       " 'e e f',\n",
       " 'e e g',\n",
       " 'e e h',\n",
       " 'e e l',\n",
       " 'e f r',\n",
       " 'e f u',\n",
       " 'e g h',\n",
       " 'e g n',\n",
       " 'e g u',\n",
       " 'e h',\n",
       " 'e h e',\n",
       " 'e h u',\n",
       " 'e j',\n",
       " 'e k e',\n",
       " 'e k n',\n",
       " 'e l f',\n",
       " 'e l r',\n",
       " 'e l v',\n",
       " 'e n e',\n",
       " 'e n k',\n",
       " 'e n l',\n",
       " 'e n n',\n",
       " 'e n u',\n",
       " 'e p',\n",
       " 'e p n',\n",
       " 'e p u',\n",
       " 'e r',\n",
       " 'e r c',\n",
       " 'e r e',\n",
       " 'e r h',\n",
       " 'e r l',\n",
       " 'e r n',\n",
       " 'e u g',\n",
       " 'e u p',\n",
       " 'e w r',\n",
       " 'e x',\n",
       " 'e x c',\n",
       " 'f b r',\n",
       " 'f e',\n",
       " 'f e b',\n",
       " 'f e l',\n",
       " 'f e n',\n",
       " 'f f',\n",
       " 'f g',\n",
       " 'f g e',\n",
       " 'f g g',\n",
       " 'f g h',\n",
       " 'f g r',\n",
       " 'f g u',\n",
       " 'f l h',\n",
       " 'f l k',\n",
       " 'f l p',\n",
       " 'f n',\n",
       " 'f n u',\n",
       " 'f r g',\n",
       " 'f r n',\n",
       " 'f u',\n",
       " 'f u c',\n",
       " 'f u z',\n",
       " 'f x',\n",
       " 'g b e',\n",
       " 'g b u',\n",
       " 'g e',\n",
       " 'g e r',\n",
       " 'g e u',\n",
       " 'g e w',\n",
       " 'g f u',\n",
       " 'g g',\n",
       " 'g g b',\n",
       " 'g g e',\n",
       " 'g g g',\n",
       " 'g g h',\n",
       " 'g g n',\n",
       " 'g g u',\n",
       " 'g h',\n",
       " 'g h e',\n",
       " 'g h h',\n",
       " 'g h n',\n",
       " 'g h u',\n",
       " 'g l k',\n",
       " 'g l l',\n",
       " 'g l n',\n",
       " 'g n',\n",
       " 'g n e',\n",
       " 'g n g',\n",
       " 'g n r',\n",
       " 'g p',\n",
       " 'g p u',\n",
       " 'g r',\n",
       " 'g r l',\n",
       " 'g u n',\n",
       " 'g w',\n",
       " 'h b e',\n",
       " 'h c',\n",
       " 'h c h',\n",
       " 'h e e',\n",
       " 'h e f',\n",
       " 'h e h',\n",
       " 'h e n',\n",
       " 'h e p',\n",
       " 'h e r',\n",
       " 'h e u',\n",
       " 'h e w',\n",
       " 'h g',\n",
       " 'h h',\n",
       " 'h h e',\n",
       " 'h h p',\n",
       " 'h j',\n",
       " 'h l',\n",
       " 'h l l',\n",
       " 'h n b',\n",
       " 'h n g',\n",
       " 'h n k',\n",
       " 'h n n',\n",
       " 'h p',\n",
       " 'h p h',\n",
       " 'h p p',\n",
       " 'h p u',\n",
       " 'h r',\n",
       " 'h r e',\n",
       " 'h r l',\n",
       " 'h r r',\n",
       " 'h u',\n",
       " 'h u g',\n",
       " 'h u n',\n",
       " 'h u r',\n",
       " 'h w',\n",
       " 'h w g',\n",
       " 'h w l',\n",
       " 'h w n',\n",
       " 'j',\n",
       " 'j e w',\n",
       " 'j g',\n",
       " 'j h',\n",
       " 'j n',\n",
       " 'k e',\n",
       " 'k e e',\n",
       " 'k e f',\n",
       " 'k e r',\n",
       " 'k f',\n",
       " 'k h',\n",
       " 'k k',\n",
       " 'k l',\n",
       " 'k l k',\n",
       " 'k l l',\n",
       " 'k n',\n",
       " 'k n l',\n",
       " 'k r',\n",
       " 'k u f',\n",
       " 'k u n',\n",
       " 'l b',\n",
       " 'l b u',\n",
       " 'l c',\n",
       " 'l c e',\n",
       " 'l e',\n",
       " 'l e b',\n",
       " 'l e h',\n",
       " 'l e n',\n",
       " 'l e p',\n",
       " 'l e u',\n",
       " 'l f',\n",
       " 'l f u',\n",
       " 'l f w',\n",
       " 'l g',\n",
       " 'l g h',\n",
       " 'l h e',\n",
       " 'l k',\n",
       " 'l k e',\n",
       " 'l l',\n",
       " 'l l c',\n",
       " 'l l e',\n",
       " 'l l h',\n",
       " 'l l l',\n",
       " 'l l v',\n",
       " 'l l w',\n",
       " 'l n',\n",
       " 'l n c',\n",
       " 'l n e',\n",
       " 'l p',\n",
       " 'l p e',\n",
       " 'l p l',\n",
       " 'l r',\n",
       " 'l r h',\n",
       " 'l u',\n",
       " 'l v',\n",
       " 'l v e',\n",
       " 'l w',\n",
       " 'l w e',\n",
       " 'l w h',\n",
       " 'n b n',\n",
       " 'n c',\n",
       " 'n c c',\n",
       " 'n c k',\n",
       " 'n c p',\n",
       " 'n c r',\n",
       " 'n e',\n",
       " 'n e b',\n",
       " 'n e c',\n",
       " 'n e g',\n",
       " 'n e l',\n",
       " 'n e r',\n",
       " 'n e v',\n",
       " 'n e w',\n",
       " 'n f',\n",
       " 'n f r',\n",
       " 'n f u',\n",
       " 'n g b',\n",
       " 'n g e',\n",
       " 'n g g',\n",
       " 'n g n',\n",
       " 'n h',\n",
       " 'n h b',\n",
       " 'n h e',\n",
       " 'n h f',\n",
       " 'n h w',\n",
       " 'n k',\n",
       " 'n k e',\n",
       " 'n k h',\n",
       " 'n k u',\n",
       " 'n l b',\n",
       " 'n l h',\n",
       " 'n l l',\n",
       " 'n n g',\n",
       " 'n n h',\n",
       " 'n n l',\n",
       " 'n n n',\n",
       " 'n n u',\n",
       " 'n p',\n",
       " 'n r',\n",
       " 'n r h',\n",
       " 'n r n',\n",
       " 'n w',\n",
       " 'n w h',\n",
       " 'n z',\n",
       " 'p b',\n",
       " 'p c',\n",
       " 'p e',\n",
       " 'p e b',\n",
       " 'p f',\n",
       " 'p f u',\n",
       " 'p h',\n",
       " 'p h e',\n",
       " 'p h n',\n",
       " 'p l',\n",
       " 'p l e',\n",
       " 'p l n',\n",
       " 'p n u',\n",
       " 'p p h',\n",
       " 'p p n',\n",
       " 'p p p',\n",
       " 'p r',\n",
       " 'p r e',\n",
       " 'p r l',\n",
       " 'p u',\n",
       " 'p u n',\n",
       " 'p u r',\n",
       " 'q u e',\n",
       " 'r b',\n",
       " 'r b c',\n",
       " 'r c',\n",
       " 'r c e',\n",
       " 'r c r',\n",
       " 'r e',\n",
       " 'r e c',\n",
       " 'r e k',\n",
       " 'r e l',\n",
       " 'r e r',\n",
       " 'r e u',\n",
       " 'r f',\n",
       " 'r f r',\n",
       " 'r g',\n",
       " 'r g r',\n",
       " 'r h',\n",
       " 'r h c',\n",
       " 'r h e',\n",
       " 'r h h',\n",
       " 'r h n',\n",
       " 'r h u',\n",
       " 'r j',\n",
       " 'r k',\n",
       " 'r k n',\n",
       " 'r l',\n",
       " 'r l e',\n",
       " 'r l n',\n",
       " 'r n',\n",
       " 'r n g',\n",
       " 'r n j',\n",
       " 'r n n',\n",
       " 'r n r',\n",
       " 'r p',\n",
       " 'r p e',\n",
       " 'r p l',\n",
       " 'r r',\n",
       " 'r r c',\n",
       " 'r r e',\n",
       " 'r r n',\n",
       " 'r v',\n",
       " 'r w',\n",
       " 'r w n',\n",
       " 'r w w',\n",
       " 'r z',\n",
       " 'u b',\n",
       " 'u b b',\n",
       " 'u b l',\n",
       " 'u c',\n",
       " 'u c e',\n",
       " 'u e e',\n",
       " 'u e f',\n",
       " 'u e r',\n",
       " 'u e w',\n",
       " 'u f',\n",
       " 'u g',\n",
       " 'u g f',\n",
       " 'u g l',\n",
       " 'u h',\n",
       " 'u h e',\n",
       " 'u h v',\n",
       " 'u k',\n",
       " 'u l',\n",
       " 'u l b',\n",
       " 'u l f',\n",
       " 'u l l',\n",
       " 'u l w',\n",
       " 'u n',\n",
       " 'u n r',\n",
       " 'u r',\n",
       " 'u r c',\n",
       " 'u r e',\n",
       " 'u r f',\n",
       " 'u r h',\n",
       " 'u r p',\n",
       " 'u u',\n",
       " 'u u p',\n",
       " 'u u u',\n",
       " 'u w',\n",
       " 'u w r',\n",
       " 'v',\n",
       " 'v c',\n",
       " 'v e l',\n",
       " 'v e n',\n",
       " 'v e r',\n",
       " 'v l',\n",
       " 'v n',\n",
       " 'w b',\n",
       " 'w b e',\n",
       " 'w c',\n",
       " 'w c h',\n",
       " 'w e b',\n",
       " 'w e e',\n",
       " 'w e l',\n",
       " 'w e r',\n",
       " 'w g r',\n",
       " 'w h e',\n",
       " 'w h g',\n",
       " 'w l',\n",
       " 'w l l',\n",
       " 'w n',\n",
       " 'w n e',\n",
       " 'w p',\n",
       " 'w r',\n",
       " 'w r e',\n",
       " 'w r r',\n",
       " 'w u l',\n",
       " 'w u p',\n",
       " 'w w e',\n",
       " 'w w u',\n",
       " 'x',\n",
       " 'x u',\n",
       " 'x x',\n",
       " 'z',\n",
       " 'z z e',\n",
       " 'IN JJ NN',\n",
       " 'JJ NN JJ',\n",
       " 'JJ VBP NN',\n",
       " 'NN DT JJ',\n",
       " 'NN JJ VBP',\n",
       " 'NN NN DT',\n",
       " 'NN VBP DT',\n",
       " 'VBD DT NN',\n",
       " 'VBP DT',\n",
       " 'VBP JJ NN',\n",
       " 'VBZ DT JJ',\n",
       " 'FKRA',\n",
       " 'FRE',\n",
       " 'num_syllables',\n",
       " 'num_chars',\n",
       " 'num_chars_total',\n",
       " 'num_terms',\n",
       " 'num_words',\n",
       " 'num_unique_words',\n",
       " 'vader neu',\n",
       " 'vader compound',\n",
       " 'num_hashtags',\n",
       " 'num_mentions',\n",
       " 'num_urls']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b b', 'b c', 'b c h', 'b e e', 'b e g', 'b e l', 'b e n', 'b e v', 'b h n', 'b l', 'b l c', 'b l l', 'b n', 'b n b', 'b n k', 'b n n', 'b r', 'b r e', 'b r f', 'b r h', 'b r n', 'b r w', 'b u r', 'c e', 'c e h', 'c e n', 'c e r', 'c h e', 'c h g', 'c h n', 'c h r', 'c k c', 'c k e', 'c k f', 'c k h', 'c k n', 'c k u', 'c l', 'c l l', 'c l r', 'c n', 'c n f', 'c n n', 'c n p', 'c n r', 'c p', 'c r', 'c r c', 'c r e', 'c r h', 'c r w', 'c u', 'c u l', 'c u n', 'c u z', 'c w', 'e b', 'e b r', 'e b u', 'e c', 'e c h', 'e c k', 'e e', 'e e b', 'e e f', 'e e g', 'e e h', 'e e l', 'e f r', 'e f u', 'e g h', 'e g n', 'e g u', 'e h', 'e h e', 'e h u', 'e j', 'e k e', 'e k n', 'e l f', 'e l r', 'e l v', 'e n e', 'e n k', 'e n l', 'e n n', 'e n u', 'e p', 'e p n', 'e p u', 'e r', 'e r c', 'e r e', 'e r h', 'e r l', 'e r n', 'e u g', 'e u p', 'e w h', 'e w r', 'e x', 'e x c', 'f b r', 'f e', 'f e b', 'f e l', 'f e n', 'f f', 'f g', 'f g e', 'f g g', 'f g h', 'f g r', 'f g u', 'f l h', 'f l k', 'f l p', 'f n', 'f n u', 'f r g', 'f r n', 'f u', 'f u c', 'f u z', 'f x', 'g b e', 'g b u', 'g e', 'g e r', 'g e u', 'g e w', 'g f u', 'g g', 'g g b', 'g g e', 'g g g', 'g g h', 'g g n', 'g g u', 'g h', 'g h e', 'g h h', 'g h n', 'g h u', 'g l k', 'g l l', 'g l n', 'g n', 'g n e', 'g n g', 'g n r', 'g p', 'g p u', 'g r', 'g r l', 'g u n', 'g w', 'h b e', 'h c', 'h c h', 'h e e', 'h e f', 'h e h', 'h e n', 'h e p', 'h e r', 'h e u', 'h e w', 'h g', 'h h', 'h h e', 'h h p', 'h j', 'h l', 'h l l', 'h n b', 'h n g', 'h n k', 'h n n', 'h p', 'h p h', 'h p p', 'h p u', 'h r', 'h r e', 'h r l', 'h r r', 'h u', 'h u g', 'h u n', 'h u r', 'h w', 'h w g', 'h w l', 'h w n', 'j', 'j e w', 'j g', 'j h', 'j n', 'k e', 'k e e', 'k e f', 'k e r', 'k f', 'k h', 'k k', 'k l', 'k l k', 'k l l', 'k n', 'k n l', 'k r', 'k u f', 'k u n', 'l b', 'l b u', 'l c', 'l c e', 'l e', 'l e b', 'l e h', 'l e n', 'l e p', 'l e u', 'l f', 'l f u', 'l f w', 'l g', 'l g h', 'l h e', 'l k', 'l k e', 'l l', 'l l c', 'l l e', 'l l h', 'l l l', 'l l v', 'l l w', 'l n', 'l n c', 'l n e', 'l p', 'l p e', 'l p l', 'l r', 'l r h', 'l u', 'l v', 'l v e', 'l w', 'l w e', 'l w h', 'n b n', 'n c', 'n c c', 'n c k', 'n c p', 'n c r', 'n e', 'n e b', 'n e c', 'n e g', 'n e l', 'n e r', 'n e v', 'n e w', 'n f', 'n f r', 'n f u', 'n g b', 'n g e', 'n g g', 'n g n', 'n h', 'n h b', 'n h e', 'n h f', 'n h w', 'n k', 'n k e', 'n k h', 'n k u', 'n l b', 'n l h', 'n l l', 'n n g', 'n n h', 'n n l', 'n n n', 'n n u', 'n p', 'n r', 'n r h', 'n r n', 'n w', 'n w h', 'n z', 'p b', 'p c', 'p e', 'p e b', 'p f', 'p f u', 'p h', 'p h e', 'p h n', 'p l', 'p l e', 'p l n', 'p n u', 'p p h', 'p p n', 'p p p', 'p r', 'p r e', 'p r l', 'p u', 'p u n', 'p u r', 'q u e', 'r b', 'r b c', 'r c', 'r c e', 'r c r', 'r e', 'r e c', 'r e k', 'r e l', 'r e r', 'r e u', 'r f', 'r f r', 'r g', 'r g r', 'r h', 'r h c', 'r h e', 'r h h', 'r h n', 'r h u', 'r j', 'r k', 'r k n', 'r l', 'r l e', 'r l n', 'r n', 'r n g', 'r n j', 'r n n', 'r n r', 'r p', 'r p e', 'r p l', 'r r', 'r r c', 'r r e', 'r r n', 'r v', 'r w', 'r w n', 'r w w', 'r z', 'u b', 'u b b', 'u b l', 'u c', 'u c e', 'u e e', 'u e f', 'u e r', 'u e w', 'u f', 'u g', 'u g f', 'u g l', 'u h', 'u h e', 'u h v', 'u k', 'u l', 'u l b', 'u l f', 'u l l', 'u l w', 'u n', 'u n r', 'u r', 'u r c', 'u r e', 'u r f', 'u r h', 'u r p', 'u u', 'u u p', 'u u u', 'u w', 'u w r', 'v', 'v c', 'v e l', 'v e n', 'v e r', 'v l', 'v n', 'w b', 'w b e', 'w c', 'w c h', 'w e b', 'w e e', 'w e l', 'w e r', 'w g r', 'w h e', 'w h g', 'w l', 'w l l', 'w n', 'w n e', 'w p', 'w r', 'w r e', 'w r r', 'w u l', 'w u p', 'w w e', 'w w u', 'x', 'x u', 'x x', 'z', 'z z e', 'IN JJ NN', 'JJ NN JJ', 'JJ VBP NN', 'NN DT JJ', 'NN JJ VBP', 'NN NN DT', 'NN VBP DT', 'VBD DT NN', 'VBP DT', 'VBP JJ NN', 'VBZ DT JJ', 'FKRA', 'FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls']\n"
     ]
    }
   ],
   "source": [
    "print (final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting names for each class of features\n",
    "ngram_features = final_feature_list[:final_feature_list.index('z z e')+ 1]\n",
    "pos_features = final_feature_list[final_feature_list.index('z z e')+1:final_feature_list.index('VBZ DT JJ')+1]\n",
    "oth_features = final_feature_list[final_feature_list.index('FKRA')+1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z z e'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {v:i for i, v in enumerate(ngram_features)}\n",
    "new_vocab_to_index = {}\n",
    "for k in ngram_features:\n",
    "    new_vocab_to_index[k] = vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of text features\n",
    "ngram_indices = final_features[:len(ngram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle new vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../py3models/final_tfidf.pkl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(new_vectorizer, '../py3models/final_tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf_ = new_vectorizer.fit_transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 2., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.7828804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 3.13038873, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 3.24660201, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 3.19928456, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.70363908, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 4.00151907, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 3.99263012, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       4.07650155, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.7168045 , 4.05144624, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 3.69195956, 5.6625921 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 4.13412325, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 4.63801144,\n",
       "       0.        , 5.71535784, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 3.33886457, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 3.02506219, 4.16805676, 3.20147035, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 3.94395489,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       2.97062495, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.39177416138165"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are the same if use IDF but the problem is that IDF will be different if we use different data. Instead we have to use the original IDF scores and multiply them by the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vals_ = idf_vals[ngram_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../py3models/final_idf.pkl']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Pickle idf_vals\n",
    "\n",
    "joblib.dump(idf_vals_, '../py3models/final_idf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_[1,:]*idf_vals_) == X_[1,:153] #Got same value as final process array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_*idf_vals_ == X_[:,:153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffinal = tfidf_*idf_vals_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating POS features\n",
    "This is simpler as we do not need to worry about IDF but it will be slower as we have to compute the POS tags for the new data. Here we can simply use the old POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = {v:i for i, v in enumerate(pos_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle pos vectorizer\n",
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "new_pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../py3models/final_pos.pkl']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(new_pos_vectorizer, '../py3models/final_pos.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = new_pos_vectorizer.fit_transform(tweet_tags).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 2.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 4.07650155, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\obloc_000\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:] == X_[:,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89273.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21620.547459814832"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[:,153:159].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can look at the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neg', 'vader pos', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls', 'is_retweet']\n"
     ]
    }
   ],
   "source": [
    "print( other_features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FRE', 'num_syllables', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls']\n"
     ]
    }
   ],
   "source": [
    "print (oth_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions can be modified to only calculate and return necessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array_(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats_ = get_feature_array_(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6.5   ,   93.14  ,   26.1   ,  127.    ,  140.    ,   25.    ,\n",
       "         25.    ,   23.    ,    0.4563,    0.    ,    1.    ])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6.5   ,   93.14  ,   26.1   ,  127.    ,  140.    ,   25.    ,\n",
       "         25.    ,   23.    ,    0.4563,    0.    ,    1.    ])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,159:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ..., \n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[:,:] == X_[:,159:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have put it all together using a simplified process we can assess if these new data return the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.concatenate([tfidffinal, pos_, feats_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24802, 170)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = pd.DataFrame(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_ = model.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.57      0.51      1431\n",
      "          1       0.97      0.92      0.94     19206\n",
      "          2       0.82      0.96      0.89      4165\n",
      "\n",
      "avg / total       0.91      0.90      0.91     24802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So now that we have verified that the results are the same with X_ and X__ we can implement a script that can transform new data in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
