{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to generate the finalized version of the classifier, to simply feature transformation into the final form, and to test that the results are the same\n",
    "\n",
    "Most of the code comes from operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephengriggs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stephengriggs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "# from joblib import *\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "nltk.download('punkt')    \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#Loading raw data\n",
    "df = pd.read_csv(\"data/labeled_data.csv\")\n",
    "tweets = df.tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24783"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Original Version\n",
    "\n",
    "# def preprocess(text_string):\n",
    "#     \"\"\"\n",
    "#     Accepts a text string and replaces:\n",
    "#     1) urls with URLHERE\n",
    "#     2) lots of whitespace with one instance\n",
    "#     3) mentions with MENTIONHERE\n",
    "\n",
    "#     This allows us to get standardized counts of urls and mentions\n",
    "#     Without caring about specific people mentioned\n",
    "#     \"\"\"\n",
    "#     space_pattern = '\\s+'\n",
    "#     giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "#         '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "#     mention_regex = '@[\\w\\-]+'\n",
    "#     parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "#     parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "#     parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "#     #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "#     return parsed_text\n",
    "\n",
    "\n",
    "## Omari's version\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "       '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-\\:]+' #<<<<<<added the semicolon after the + to remove : at end of Rt's\n",
    "    emoji_regex = '&#[0-9\\;\\:]+'    #<<<<<<<<<remove emoji's .ex; &#1214324\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(emoji_regex,'',parsed_text)\n",
    "    parsed_text = parsed_text.strip(string.punctuation)\n",
    "    return parsed_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Original Version \n",
    "# def tokenize(tweet):\n",
    "#     \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "#     and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "#     tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "#     #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "#     tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "\n",
    "## Omari's Version used for initial models\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split('\\s|(?<!\\d)[,.]|[,.](?!\\d)', tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "## Kelly's version\n",
    "# def tokenize(tweet):\n",
    "#     tokens = []\n",
    "#     # remove non-alphabetic characters\n",
    "#     tweet_text = tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(tweet))\n",
    "#     #remove html content\n",
    "#     tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "#     # tokenize\n",
    "#     words = word_tokenize(tweet_text.lower())\n",
    "#     # lemmatize each word to its lemma\n",
    "#     lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "#     tokens.append(lemma_words)\n",
    "#     return(tokens[0])\n",
    "\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24783, 7271)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '! @@@@': 1,\n",
       " '! @@@@ top': 2,\n",
       " '!!': 3,\n",
       " '!!!': 4,\n",
       " '!!!!': 5,\n",
       " '\"': 6,\n",
       " '\" \"': 7,\n",
       " '\" -': 8,\n",
       " '\" bitch': 9,\n",
       " '\" damn': 10,\n",
       " '\" fuck': 11,\n",
       " '\" got': 12,\n",
       " '\" hoe': 13,\n",
       " '\" i\\'m': 14,\n",
       " '\" like': 15,\n",
       " '\" lmao': 16,\n",
       " '\" love': 17,\n",
       " '\" nigga': 18,\n",
       " '\" pussi': 19,\n",
       " '\" swear': 20,\n",
       " '\" thi': 21,\n",
       " '\" thi bitch': 22,\n",
       " '\"a': 23,\n",
       " '\"all': 24,\n",
       " '\"bad': 25,\n",
       " '\"bad bitch\"': 26,\n",
       " '\"bad bitches\"': 27,\n",
       " '\"be': 28,\n",
       " '\"bitch': 29,\n",
       " '\"bitch\"': 30,\n",
       " '\"black': 31,\n",
       " '\"come': 32,\n",
       " '\"damn': 33,\n",
       " '\"do': 34,\n",
       " '\"don\\'t': 35,\n",
       " '\"fuck': 36,\n",
       " '\"fuck right': 37,\n",
       " '\"girl': 38,\n",
       " '\"go': 39,\n",
       " '\"go talk': 40,\n",
       " '\"go talk hoes\"': 41,\n",
       " '\"good': 42,\n",
       " '\"he': 43,\n",
       " '\"hey': 44,\n",
       " '\"hoe': 45,\n",
       " '\"hoes\"': 46,\n",
       " '\"i': 47,\n",
       " '\"i ain\\'t': 48,\n",
       " '\"i got': 49,\n",
       " '\"i hate': 50,\n",
       " '\"i love': 51,\n",
       " '\"i play': 52,\n",
       " '\"i play soccer': 53,\n",
       " '\"i want': 54,\n",
       " '\"i\\'m': 55,\n",
       " '\"if': 56,\n",
       " '\"in': 57,\n",
       " '\"it': 58,\n",
       " '\"it\\'': 59,\n",
       " '\"let': 60,\n",
       " '\"look': 61,\n",
       " '\"mi': 62,\n",
       " '\"nah': 63,\n",
       " '\"new': 64,\n",
       " '\"nigga': 65,\n",
       " '\"nigger\"': 66,\n",
       " '\"no': 67,\n",
       " '\"oh': 68,\n",
       " '\"omg': 69,\n",
       " '\"pussi': 70,\n",
       " '\"she': 71,\n",
       " '\"so': 72,\n",
       " '\"stop': 73,\n",
       " '\"that': 74,\n",
       " '\"that\\'': 75,\n",
       " '\"the': 76,\n",
       " '\"these': 77,\n",
       " '\"these hoe': 78,\n",
       " '\"thi': 79,\n",
       " '\"thi bitch': 80,\n",
       " '\"u': 81,\n",
       " '\"we': 82,\n",
       " '\"well': 83,\n",
       " '\"what': 84,\n",
       " '\"where': 85,\n",
       " '\"whi': 86,\n",
       " '\"who': 87,\n",
       " '\"you': 88,\n",
       " '\"you might': 89,\n",
       " '\"you might ghetto': 90,\n",
       " '\"you think': 91,\n",
       " '\"you\\'r': 92,\n",
       " '\"your': 93,\n",
       " '#': 94,\n",
       " '#1': 95,\n",
       " '#300': 96,\n",
       " '#8221': 97,\n",
       " '#8221;': 98,\n",
       " '#8221; bitch': 99,\n",
       " '#8221; thi': 100,\n",
       " '#8221;\"': 101,\n",
       " '#8230': 102,\n",
       " '#android': 103,\n",
       " '#android #iphon': 104,\n",
       " '#android #iphon #ipad': 105,\n",
       " '#astro': 106,\n",
       " '#bird': 107,\n",
       " '#bitch': 108,\n",
       " '#cle': 109,\n",
       " '#cunt': 110,\n",
       " '#emabiggestfans1d': 111,\n",
       " '#fact': 112,\n",
       " '#fag': 113,\n",
       " '#faggot': 114,\n",
       " '#farewellcaptain': 115,\n",
       " '#ferguson': 116,\n",
       " '#foxnew': 117,\n",
       " '#gay': 118,\n",
       " '#gop': 119,\n",
       " '#hoe': 120,\n",
       " '#hoosier': 121,\n",
       " '#htgawm': 122,\n",
       " '#indiana': 123,\n",
       " '#ipad': 124,\n",
       " '#ipad #sex': 125,\n",
       " '#ipad #sex #xxx': 126,\n",
       " '#iphon': 127,\n",
       " '#iphon #ipad': 128,\n",
       " '#iphon #ipad #sex': 129,\n",
       " '#iraq': 130,\n",
       " '#isi': 131,\n",
       " '#iu': 132,\n",
       " '#iubb': 133,\n",
       " '#iubb #hoosier': 134,\n",
       " '#ladykimora': 135,\n",
       " '#lol': 136,\n",
       " '#losangel': 137,\n",
       " '#mlb': 138,\n",
       " '#morningjo': 139,\n",
       " '#nba': 140,\n",
       " '#ncaa': 141,\n",
       " '#nfl': 142,\n",
       " '#obama': 143,\n",
       " '#p2': 144,\n",
       " '#pjnet': 145,\n",
       " '#pussi': 146,\n",
       " '#realtalk': 147,\n",
       " '#redskin': 148,\n",
       " '#rejectedpeanutsspeci': 149,\n",
       " '#retard': 150,\n",
       " '#salut': 151,\n",
       " '#saudi': 152,\n",
       " '#scandal': 153,\n",
       " '#sex': 154,\n",
       " '#sex #xxx': 155,\n",
       " '#sex #xxx |': 156,\n",
       " '#stribpol': 157,\n",
       " '#syria': 158,\n",
       " '#tbt': 159,\n",
       " '#tcot': 160,\n",
       " '#tcot #pjnet': 161,\n",
       " '#tcot #teaparti': 162,\n",
       " '#teabagg': 163,\n",
       " '#teaparti': 164,\n",
       " '#trash': 165,\n",
       " '#truth': 166,\n",
       " '#vote5so': 167,\n",
       " '#wcvb': 168,\n",
       " '#wcw': 169,\n",
       " '#wtf': 170,\n",
       " '#xxx': 171,\n",
       " '#xxx |': 172,\n",
       " '#yanke': 173,\n",
       " '#yanke #mlb': 174,\n",
       " '$200': 175,\n",
       " '&': 176,\n",
       " '&amp;': 177,\n",
       " '&amp; 2': 178,\n",
       " '&amp; bitch': 179,\n",
       " '&amp; fuck': 180,\n",
       " '&amp; get': 181,\n",
       " '&amp; go': 182,\n",
       " '&amp; got': 183,\n",
       " '&amp; hi': 184,\n",
       " '&amp; hoe': 185,\n",
       " \"&amp; i'm\": 186,\n",
       " '&amp; like': 187,\n",
       " '&amp; nigga': 188,\n",
       " '&amp; still': 189,\n",
       " '&amp; take': 190,\n",
       " '&amp; throw': 191,\n",
       " '&amp; u': 192,\n",
       " '&amp; wa': 193,\n",
       " '&amp; white': 194,\n",
       " '&amp;&amp;': 195,\n",
       " '&gt;': 196,\n",
       " '&lt;3': 197,\n",
       " \"'\": 198,\n",
       " \"'blue\": 199,\n",
       " \"'blue clues'\": 200,\n",
       " \"'blue clues' left\": 201,\n",
       " \"'bout\": 202,\n",
       " \"'caus\": 203,\n",
       " \"'em\": 204,\n",
       " \"'s\": 205,\n",
       " '(': 206,\n",
       " '( )': 207,\n",
       " '(cont)': 208,\n",
       " '(via': 209,\n",
       " ')': 210,\n",
       " '*hit': 211,\n",
       " '*hit blunt*': 212,\n",
       " '+': 213,\n",
       " '-': 214,\n",
       " '- 24': 215,\n",
       " '- hot': 216,\n",
       " '- hot air': 217,\n",
       " '--': 218,\n",
       " '---': 219,\n",
       " '--top': 220,\n",
       " '--top rate': 221,\n",
       " '--top rate spa': 222,\n",
       " '-622-0221': 223,\n",
       " '-622-0221 -': 224,\n",
       " '-622-0221 - 24': 225,\n",
       " '/': 226,\n",
       " '0': 227,\n",
       " '1': 228,\n",
       " '1)': 229,\n",
       " '1/2': 230,\n",
       " '10': 231,\n",
       " '100': 232,\n",
       " '100 million': 233,\n",
       " '100%': 234,\n",
       " '1000': 235,\n",
       " '11': 236,\n",
       " '12': 237,\n",
       " '13': 238,\n",
       " '14': 239,\n",
       " '15': 240,\n",
       " '16': 241,\n",
       " '17': 242,\n",
       " '18': 243,\n",
       " '1st': 244,\n",
       " '1st date': 245,\n",
       " '1st date dont': 246,\n",
       " '2': 247,\n",
       " '2 3': 248,\n",
       " '2 bitch': 249,\n",
       " '2 day': 250,\n",
       " '2 month': 251,\n",
       " '2 year': 252,\n",
       " '2)': 253,\n",
       " '20': 254,\n",
       " '2014': 255,\n",
       " '2015': 256,\n",
       " '21': 257,\n",
       " '24': 258,\n",
       " '24/7': 259,\n",
       " '25': 260,\n",
       " '27': 261,\n",
       " '2am': 262,\n",
       " '2k': 263,\n",
       " '2nd': 264,\n",
       " '3': 265,\n",
       " '3 4': 266,\n",
       " '3 bitch': 267,\n",
       " '3 day': 268,\n",
       " '3 time': 269,\n",
       " '3)': 270,\n",
       " '30': 271,\n",
       " '300': 272,\n",
       " '35': 273,\n",
       " '3rd': 274,\n",
       " '4': 275,\n",
       " '4.99': 276,\n",
       " '40': 277,\n",
       " '45': 278,\n",
       " '48': 279,\n",
       " '4th': 280,\n",
       " '5': 281,\n",
       " '5 minut': 282,\n",
       " '5 year': 283,\n",
       " '50': 284,\n",
       " '5so': 285,\n",
       " '5th': 286,\n",
       " '5th grade': 287,\n",
       " '6': 288,\n",
       " '60': 289,\n",
       " '64': 290,\n",
       " '6th': 291,\n",
       " '6th grade': 292,\n",
       " '7': 293,\n",
       " '70': 294,\n",
       " '718-622-0221': 295,\n",
       " '718-622-0221 -': 296,\n",
       " '718-622-0221 - 24': 297,\n",
       " '7th': 298,\n",
       " '8': 299,\n",
       " '80%': 300,\n",
       " '8th': 301,\n",
       " '9': 302,\n",
       " '9/11': 303,\n",
       " '90': 304,\n",
       " '90 day': 305,\n",
       " '99': 306,\n",
       " '99 problem': 307,\n",
       " '99 problem bitch': 308,\n",
       " ':': 309,\n",
       " ':(': 310,\n",
       " ':)': 311,\n",
       " ':/': 312,\n",
       " ':p': 313,\n",
       " ';': 314,\n",
       " ';)': 315,\n",
       " '=': 316,\n",
       " '?': 317,\n",
       " '? \"': 318,\n",
       " '? lol': 319,\n",
       " '?!': 320,\n",
       " '?\"': 321,\n",
       " '??': 322,\n",
       " '@': 323,\n",
       " '@ bitch': 324,\n",
       " '@ em': 325,\n",
       " '@@@@': 326,\n",
       " '@@@@ top': 327,\n",
       " '@@@@ top class': 328,\n",
       " \"a'\": 329,\n",
       " 'a1': 330,\n",
       " 'ab': 331,\n",
       " 'abil': 332,\n",
       " 'abl': 333,\n",
       " 'abo': 334,\n",
       " 'abort': 335,\n",
       " 'abov': 336,\n",
       " 'absolut': 337,\n",
       " 'abt': 338,\n",
       " 'abu': 339,\n",
       " 'abus': 340,\n",
       " 'accent': 341,\n",
       " 'accept': 342,\n",
       " 'accid': 343,\n",
       " 'accident': 344,\n",
       " 'accord': 345,\n",
       " 'account': 346,\n",
       " 'accur': 347,\n",
       " 'acknowledg': 348,\n",
       " 'across': 349,\n",
       " 'act': 350,\n",
       " 'act like': 351,\n",
       " 'act like bitch': 352,\n",
       " 'act like hoe': 353,\n",
       " 'act like pussi': 354,\n",
       " 'actin': 355,\n",
       " 'actin like': 356,\n",
       " 'action': 357,\n",
       " 'activ': 358,\n",
       " 'actor': 359,\n",
       " 'actual': 360,\n",
       " 'ad': 361,\n",
       " 'adam': 362,\n",
       " 'add': 363,\n",
       " 'addict': 364,\n",
       " 'address': 365,\n",
       " 'admit': 366,\n",
       " 'adopt': 367,\n",
       " 'ador': 368,\n",
       " 'adult': 369,\n",
       " 'advanc': 370,\n",
       " 'advic': 371,\n",
       " 'ae': 372,\n",
       " 'af': 373,\n",
       " 'affect': 374,\n",
       " 'afford': 375,\n",
       " 'afraid': 376,\n",
       " 'african': 377,\n",
       " 'afterward': 378,\n",
       " 'ag': 379,\n",
       " 'age': 380,\n",
       " 'ago': 381,\n",
       " 'agre': 382,\n",
       " 'ah': 383,\n",
       " 'ah bitch': 384,\n",
       " 'aha': 385,\n",
       " 'ahaha': 386,\n",
       " 'ahead': 387,\n",
       " 'ahh': 388,\n",
       " 'ahhh': 389,\n",
       " 'aid': 390,\n",
       " \"ain't\": 391,\n",
       " \"ain't bad\": 392,\n",
       " \"ain't bitch\": 393,\n",
       " \"ain't bout\": 394,\n",
       " \"ain't color\": 395,\n",
       " \"ain't even\": 396,\n",
       " \"ain't fuck\": 397,\n",
       " \"ain't fuckin\": 398,\n",
       " \"ain't get\": 399,\n",
       " \"ain't go\": 400,\n",
       " \"ain't gon\": 401,\n",
       " \"ain't gone\": 402,\n",
       " \"ain't good\": 403,\n",
       " \"ain't got\": 404,\n",
       " \"ain't got hoe\": 405,\n",
       " \"ain't gotta\": 406,\n",
       " \"ain't hoe\": 407,\n",
       " \"ain't killer\": 408,\n",
       " \"ain't killer push\": 409,\n",
       " \"ain't like\": 410,\n",
       " \"ain't loyal\": 411,\n",
       " 'ain\\'t loyal\"': 412,\n",
       " \"ain't make\": 413,\n",
       " \"ain't never\": 414,\n",
       " \"ain't nobodi\": 415,\n",
       " \"ain't noth\": 416,\n",
       " \"ain't noth cut\": 417,\n",
       " \"ain't nothin\": 418,\n",
       " \"ain't nothin cut\": 419,\n",
       " \"ain't nun\": 420,\n",
       " \"ain't nuttin\": 421,\n",
       " \"ain't one\": 422,\n",
       " \"ain't pussi\": 423,\n",
       " \"ain't real\": 424,\n",
       " \"ain't realli\": 425,\n",
       " \"ain't right\": 426,\n",
       " \"ain't shit\": 427,\n",
       " \"ain't shit hoe\": 428,\n",
       " \"ain't thi\": 429,\n",
       " \"ain't worri\": 430,\n",
       " \"ain't worri bout\": 431,\n",
       " \"ain't worth\": 432,\n",
       " 'ainn': 433,\n",
       " 'aint': 434,\n",
       " 'aint bitch': 435,\n",
       " 'aint even': 436,\n",
       " 'aint fuck': 437,\n",
       " 'aint fuckin': 438,\n",
       " 'aint get': 439,\n",
       " 'aint got': 440,\n",
       " 'aint hoe': 441,\n",
       " 'aint hoe get': 442,\n",
       " 'aint loyal': 443,\n",
       " 'aint never': 444,\n",
       " 'aint nobodi': 445,\n",
       " 'aint noth': 446,\n",
       " 'aint shit': 447,\n",
       " 'aint shit bitch': 448,\n",
       " 'air': 449,\n",
       " 'aka': 450,\n",
       " 'al': 451,\n",
       " 'alabama': 452,\n",
       " 'alarm': 453,\n",
       " 'albino': 454,\n",
       " 'album': 455,\n",
       " 'alcohol': 456,\n",
       " 'alex': 457,\n",
       " 'alex smith': 458,\n",
       " 'alien': 459,\n",
       " 'alik': 460,\n",
       " 'aliv': 461,\n",
       " 'allen': 462,\n",
       " 'allow': 463,\n",
       " 'almost': 464,\n",
       " 'almost got': 465,\n",
       " 'alon': 466,\n",
       " 'along': 467,\n",
       " 'alot': 468,\n",
       " 'alotta': 469,\n",
       " 'alpha': 470,\n",
       " 'alreadi': 471,\n",
       " 'alreadi got': 472,\n",
       " 'alreadi know': 473,\n",
       " 'alright': 474,\n",
       " 'alsina': 475,\n",
       " 'also': 476,\n",
       " 'alway': 477,\n",
       " 'alway bitch': 478,\n",
       " 'alway get': 479,\n",
       " 'alway got': 480,\n",
       " 'alway hoe': 481,\n",
       " 'alway look': 482,\n",
       " 'alway say': 483,\n",
       " 'alway talk': 484,\n",
       " 'alway tryna': 485,\n",
       " 'alway wanna': 486,\n",
       " 'alway want': 487,\n",
       " 'amanda': 488,\n",
       " 'amaz': 489,\n",
       " 'amber': 490,\n",
       " 'amber rose': 491,\n",
       " 'amen': 492,\n",
       " 'america': 493,\n",
       " 'american': 494,\n",
       " 'amo': 495,\n",
       " 'amount': 496,\n",
       " 'amp;': 497,\n",
       " 'anal': 498,\n",
       " 'anchor': 499,\n",
       " 'anchor babi': 500,\n",
       " 'angel': 501,\n",
       " 'anger': 502,\n",
       " 'angri': 503,\n",
       " 'angri bird': 504,\n",
       " 'ani': 505,\n",
       " 'ani bitch': 506,\n",
       " 'ani girl': 507,\n",
       " 'ani man': 508,\n",
       " 'ani nigga': 509,\n",
       " 'ani pussi': 510,\n",
       " 'anim': 511,\n",
       " 'anim cracker': 512,\n",
       " 'ankl': 513,\n",
       " 'ann': 514,\n",
       " 'announc': 515,\n",
       " 'annoy': 516,\n",
       " 'annoy bitch': 517,\n",
       " 'anoth': 518,\n",
       " 'anoth bitch': 519,\n",
       " 'anoth man': 520,\n",
       " 'anoth man treasur': 521,\n",
       " 'anoth nigga': 522,\n",
       " 'answer': 523,\n",
       " 'anthem': 524,\n",
       " 'anthoni': 525,\n",
       " 'anti': 526,\n",
       " 'antonio': 527,\n",
       " 'anxieti': 528,\n",
       " 'anybodi': 529,\n",
       " 'anymor': 530,\n",
       " 'anyon': 531,\n",
       " 'anyth': 532,\n",
       " 'anytim': 533,\n",
       " 'anyway': 534,\n",
       " 'anywher': 535,\n",
       " 'ap': 536,\n",
       " 'apart': 537,\n",
       " 'ape': 538,\n",
       " 'ape shit': 539,\n",
       " 'apolog': 540,\n",
       " 'app': 541,\n",
       " 'appar': 542,\n",
       " 'appear': 543,\n",
       " 'appl': 544,\n",
       " 'appli': 545,\n",
       " 'appreci': 546,\n",
       " 'approach': 547,\n",
       " 'approv': 548,\n",
       " 'arab': 549,\n",
       " 'area': 550,\n",
       " 'arent': 551,\n",
       " 'argu': 552,\n",
       " 'argu bitch': 553,\n",
       " 'argument': 554,\n",
       " 'arizona': 555,\n",
       " 'arm': 556,\n",
       " 'armi': 557,\n",
       " 'around': 558,\n",
       " 'around thi': 559,\n",
       " 'around thi bitch': 560,\n",
       " 'arrest': 561,\n",
       " 'arrog': 562,\n",
       " 'art': 563,\n",
       " 'articl': 564,\n",
       " 'artist': 565,\n",
       " 'asf': 566,\n",
       " 'asham': 567,\n",
       " 'asian': 568,\n",
       " 'asian bitch': 569,\n",
       " 'asian massag': 570,\n",
       " 'asian massag brooklyn': 571,\n",
       " 'asian massag park': 572,\n",
       " 'asid': 573,\n",
       " 'ask': 574,\n",
       " 'ask bitch': 575,\n",
       " 'ask everi': 576,\n",
       " 'ask everi bitch': 577,\n",
       " 'ask want': 578,\n",
       " 'ask whi': 579,\n",
       " 'askin': 580,\n",
       " 'asleep': 581,\n",
       " 'ass': 582,\n",
       " 'ass back': 583,\n",
       " 'ass beat': 584,\n",
       " 'ass bitch': 585,\n",
       " \"ass bitch ain't\": 586,\n",
       " 'ass bitch aint': 587,\n",
       " 'ass bitch fuck': 588,\n",
       " 'ass bitch lol': 589,\n",
       " 'ass bitch\"': 590,\n",
       " 'ass come': 591,\n",
       " 'ass cracker': 592,\n",
       " 'ass cunt': 593,\n",
       " 'ass dick': 594,\n",
       " 'ass fuck': 595,\n",
       " 'ass get': 596,\n",
       " 'ass girl': 597,\n",
       " 'ass go': 598,\n",
       " 'ass got': 599,\n",
       " 'ass hoe': 600,\n",
       " \"ass i'm\": 601,\n",
       " 'ass like': 602,\n",
       " 'ass littl': 603,\n",
       " 'ass lol': 604,\n",
       " 'ass man': 605,\n",
       " 'ass monkey': 606,\n",
       " 'ass nicca': 607,\n",
       " 'ass nigga': 608,\n",
       " 'ass nigga joe': 609,\n",
       " 'ass niggah': 610,\n",
       " 'ass nigguh': 611,\n",
       " 'ass peopl': 612,\n",
       " 'ass pussi': 613,\n",
       " 'ass u': 614,\n",
       " 'ass way': 615,\n",
       " 'ass white': 616,\n",
       " 'ass!': 617,\n",
       " 'ass?': 618,\n",
       " 'asshol': 619,\n",
       " 'associ': 620,\n",
       " 'assum': 621,\n",
       " 'at?': 622,\n",
       " 'ate': 623,\n",
       " 'ate pussi': 624,\n",
       " 'athlet': 625,\n",
       " 'atl': 626,\n",
       " 'atlanta': 627,\n",
       " 'atleast': 628,\n",
       " 'attach': 629,\n",
       " 'attack': 630,\n",
       " 'attempt': 631,\n",
       " 'attend': 632,\n",
       " 'attent': 633,\n",
       " 'attitud': 634,\n",
       " 'attract': 635,\n",
       " 'audienc': 636,\n",
       " 'august': 637,\n",
       " 'aunt': 638,\n",
       " 'austin': 639,\n",
       " 'autocorrect': 640,\n",
       " 'automat': 641,\n",
       " 'aux': 642,\n",
       " 'aux cord': 643,\n",
       " 'avail': 644,\n",
       " 'averag': 645,\n",
       " 'avi': 646,\n",
       " 'avoid': 647,\n",
       " 'aw': 648,\n",
       " 'awak': 649,\n",
       " 'awar': 650,\n",
       " 'award': 651,\n",
       " 'away': 652,\n",
       " 'awe': 653,\n",
       " 'awesom': 654,\n",
       " 'awkward': 655,\n",
       " 'awkward moment': 656,\n",
       " 'aww': 657,\n",
       " 'ay': 658,\n",
       " 'aye': 659,\n",
       " 'az': 660,\n",
       " 'azz': 661,\n",
       " 'b': 662,\n",
       " 'b bitch': 663,\n",
       " 'b/c': 664,\n",
       " 'b4': 665,\n",
       " 'babe': 666,\n",
       " 'babi': 667,\n",
       " 'babi bird': 668,\n",
       " 'babi daddi': 669,\n",
       " 'babi momma': 670,\n",
       " 'bacc': 671,\n",
       " 'back': 672,\n",
       " 'back bitch': 673,\n",
       " 'back hoe': 674,\n",
       " 'back lol': 675,\n",
       " 'back shit': 676,\n",
       " 'back thi': 677,\n",
       " 'back thi bitch': 678,\n",
       " 'back?': 679,\n",
       " 'background': 680,\n",
       " 'backpack': 681,\n",
       " 'backpag': 682,\n",
       " 'backward': 683,\n",
       " 'bacon': 684,\n",
       " 'bad': 685,\n",
       " 'bad ass': 686,\n",
       " 'bad bitch': 687,\n",
       " 'bad bitch come': 688,\n",
       " 'bad bitch give': 689,\n",
       " 'bad bitch good': 690,\n",
       " 'bad bitch onli': 691,\n",
       " \"bad bitch that'\": 692,\n",
       " 'bad built': 693,\n",
       " 'bad hoe': 694,\n",
       " 'badass': 695,\n",
       " 'baddest': 696,\n",
       " 'baddest bitch': 697,\n",
       " 'bae': 698,\n",
       " 'bae:': 699,\n",
       " 'bag': 700,\n",
       " 'bail': 701,\n",
       " 'bait': 702,\n",
       " 'bake': 703,\n",
       " 'baker': 704,\n",
       " 'bald': 705,\n",
       " 'bald head': 706,\n",
       " 'bald head bitch': 707,\n",
       " 'ball': 708,\n",
       " 'ballin': 709,\n",
       " 'baltimor': 710,\n",
       " 'ban': 711,\n",
       " 'banana': 712,\n",
       " 'band': 713,\n",
       " 'bandwagon': 714,\n",
       " 'bang': 715,\n",
       " 'banger': 716,\n",
       " 'bank': 717,\n",
       " 'bar': 718,\n",
       " 'barber': 719,\n",
       " 'bare': 720,\n",
       " 'barrel': 721,\n",
       " 'base': 722,\n",
       " 'basebal': 723,\n",
       " 'basement': 724,\n",
       " 'bash': 725,\n",
       " 'basi': 726,\n",
       " 'basic': 727,\n",
       " 'basic ass': 728,\n",
       " 'basic ass bitch': 729,\n",
       " 'basic bitch': 730,\n",
       " 'basketbal': 731,\n",
       " 'bastard': 732,\n",
       " 'bat': 733,\n",
       " 'bath': 734,\n",
       " 'bathroom': 735,\n",
       " 'battl': 736,\n",
       " 'bc': 737,\n",
       " 'bday': 738,\n",
       " 'beach': 739,\n",
       " 'bean': 740,\n",
       " 'beaner': 741,\n",
       " 'beani': 742,\n",
       " 'bear': 743,\n",
       " 'beard': 744,\n",
       " 'beast': 745,\n",
       " 'beat': 746,\n",
       " 'beat bitch': 747,\n",
       " 'beat pussi': 748,\n",
       " 'beat shit': 749,\n",
       " 'beat yo': 750,\n",
       " 'beatin': 751,\n",
       " 'beauti': 752,\n",
       " 'beauti bitch': 753,\n",
       " 'becam': 754,\n",
       " 'becaus': 755,\n",
       " 'becaus bitch': 756,\n",
       " 'becaus fuck': 757,\n",
       " \"becaus he'\": 758,\n",
       " 'becaus hoe': 759,\n",
       " \"becaus i'm\": 760,\n",
       " \"becaus it'\": 761,\n",
       " 'becaus know': 762,\n",
       " 'becaus love': 763,\n",
       " 'becaus nigga': 764,\n",
       " \"becaus they'r\": 765,\n",
       " \"becaus you'r\": 766,\n",
       " 'becom': 767,\n",
       " 'bed': 768,\n",
       " 'bee': 769,\n",
       " 'beef': 770,\n",
       " 'beer': 771,\n",
       " 'befor': 772,\n",
       " 'befor eat': 773,\n",
       " 'befor hoe': 774,\n",
       " 'beg': 775,\n",
       " 'begin': 776,\n",
       " 'behind': 777,\n",
       " 'behind back': 778,\n",
       " 'bein': 779,\n",
       " 'believ': 780,\n",
       " 'bell': 781,\n",
       " 'belli': 782,\n",
       " 'belong': 783,\n",
       " 'belt': 784,\n",
       " 'ben': 785,\n",
       " 'bench': 786,\n",
       " 'bend': 787,\n",
       " 'benz': 788,\n",
       " 'berg': 789,\n",
       " 'besid': 790,\n",
       " 'best': 791,\n",
       " 'best asian': 792,\n",
       " 'best asian massag': 793,\n",
       " 'best friend': 794,\n",
       " 'best pussi': 795,\n",
       " 'bestfriend': 796,\n",
       " 'bet': 797,\n",
       " 'bet u': 798,\n",
       " 'betray': 799,\n",
       " 'betta': 800,\n",
       " 'better': 801,\n",
       " 'better bitch': 802,\n",
       " 'better get': 803,\n",
       " 'beyonc': 804,\n",
       " 'beyond': 805,\n",
       " 'bf': 806,\n",
       " 'bffl': 807,\n",
       " 'bibl': 808,\n",
       " 'bid': 809,\n",
       " 'biden': 810,\n",
       " 'bieber': 811,\n",
       " 'big': 812,\n",
       " 'big ass': 813,\n",
       " 'big bird': 814,\n",
       " 'big bitch': 815,\n",
       " 'big booti': 816,\n",
       " 'big booti bitch': 817,\n",
       " 'big booti hoe': 818,\n",
       " 'big pussi': 819,\n",
       " 'bigger': 820,\n",
       " 'biggest': 821,\n",
       " 'biggest bitch': 822,\n",
       " 'biggest faggot': 823,\n",
       " 'biggest hoe': 824,\n",
       " 'bike': 825,\n",
       " 'bill': 826,\n",
       " 'billion': 827,\n",
       " 'bin': 828,\n",
       " 'bio': 829,\n",
       " 'bird': 830,\n",
       " 'bird bee': 831,\n",
       " 'bird catch': 832,\n",
       " 'bird catch worm': 833,\n",
       " 'bird feather': 834,\n",
       " 'bird feather flock': 835,\n",
       " 'bird fli': 836,\n",
       " 'bird flu': 837,\n",
       " 'bird get': 838,\n",
       " 'bird get worm': 839,\n",
       " 'bird lol': 840,\n",
       " 'bird make': 841,\n",
       " 'bird man': 842,\n",
       " 'bird one': 843,\n",
       " 'bird outsid': 844,\n",
       " 'bird shit': 845,\n",
       " 'bird!': 846,\n",
       " 'bird?': 847,\n",
       " 'birth': 848,\n",
       " 'birth control': 849,\n",
       " 'birthday': 850,\n",
       " 'birthday bitch': 851,\n",
       " 'bit': 852,\n",
       " 'bitch': 853,\n",
       " 'bitch !': 854,\n",
       " 'bitch \"': 855,\n",
       " 'bitch #8221': 856,\n",
       " 'bitch #8221;': 857,\n",
       " 'bitch &amp;': 858,\n",
       " 'bitch 2': 859,\n",
       " 'bitch ?': 860,\n",
       " 'bitch act': 861,\n",
       " 'bitch act like': 862,\n",
       " 'bitch actin': 863,\n",
       " 'bitch actin like': 864,\n",
       " \"bitch ain't\": 865,\n",
       " \"bitch ain't fuckin\": 866,\n",
       " \"bitch ain't one\": 867,\n",
       " \"bitch ain't shit\": 868,\n",
       " 'bitch aint': 869,\n",
       " 'bitch aint shit': 870,\n",
       " 'bitch almost': 871,\n",
       " 'bitch alreadi': 872,\n",
       " 'bitch alway': 873,\n",
       " 'bitch answer': 874,\n",
       " 'bitch anyth': 875,\n",
       " 'bitch ask': 876,\n",
       " 'bitch ass': 877,\n",
       " 'bitch ass nigga': 878,\n",
       " 'bitch attitud': 879,\n",
       " 'bitch b': 880,\n",
       " 'bitch babi': 881,\n",
       " 'bitch back': 882,\n",
       " 'bitch bad': 883,\n",
       " 'bitch basic': 884,\n",
       " 'bitch bc': 885,\n",
       " 'bitch beauti': 886,\n",
       " 'bitch becaus': 887,\n",
       " 'bitch befor': 888,\n",
       " 'bitch bet': 889,\n",
       " 'bitch better': 890,\n",
       " 'bitch big': 891,\n",
       " 'bitch bitch': 892,\n",
       " 'bitch bitch bitch': 893,\n",
       " 'bitch blew': 894,\n",
       " 'bitch blew high': 895,\n",
       " 'bitch blow': 896,\n",
       " 'bitch bout': 897,\n",
       " 'bitch boy': 898,\n",
       " 'bitch bring': 899,\n",
       " 'bitch broke': 900,\n",
       " 'bitch bruh': 901,\n",
       " 'bitch buy': 902,\n",
       " 'bitch bye': 903,\n",
       " 'bitch call': 904,\n",
       " 'bitch came': 905,\n",
       " \"bitch can't\": 906,\n",
       " \"bitch can't handl\": 907,\n",
       " 'bitch cant': 908,\n",
       " 'bitch car': 909,\n",
       " 'bitch caus': 910,\n",
       " 'bitch cheat': 911,\n",
       " 'bitch choosin': 912,\n",
       " 'bitch club': 913,\n",
       " 'bitch come': 914,\n",
       " 'bitch complain': 915,\n",
       " 'bitch corni': 916,\n",
       " 'bitch could': 917,\n",
       " 'bitch crazi': 918,\n",
       " 'bitch cri': 919,\n",
       " 'bitch cut': 920,\n",
       " 'bitch cuz': 921,\n",
       " 'bitch da': 922,\n",
       " 'bitch damn': 923,\n",
       " 'bitch dat': 924,\n",
       " 'bitch day': 925,\n",
       " 'bitch dead': 926,\n",
       " 'bitch deserv': 927,\n",
       " 'bitch dick': 928,\n",
       " 'bitch die': 929,\n",
       " 'bitch dm': 930,\n",
       " 'bitch doe': 931,\n",
       " 'bitch done': 932,\n",
       " 'bitch dont': 933,\n",
       " 'bitch dont like': 934,\n",
       " 'bitch dress': 935,\n",
       " 'bitch drive': 936,\n",
       " 'bitch dude': 937,\n",
       " 'bitch dumb': 938,\n",
       " 'bitch ear': 939,\n",
       " 'bitch eat': 940,\n",
       " 'bitch either': 941,\n",
       " 'bitch even': 942,\n",
       " 'bitch ever': 943,\n",
       " 'bitch everi': 944,\n",
       " 'bitch everyth': 945,\n",
       " 'bitch face': 946,\n",
       " 'bitch fact': 947,\n",
       " 'bitch fake': 948,\n",
       " 'bitch fat': 949,\n",
       " 'bitch feel': 950,\n",
       " 'bitch feet': 951,\n",
       " 'bitch fight': 952,\n",
       " 'bitch find': 953,\n",
       " 'bitch finna': 954,\n",
       " 'bitch foh': 955,\n",
       " 'bitch follow': 956,\n",
       " 'bitch fuc': 957,\n",
       " 'bitch fuck': 958,\n",
       " 'bitch fuck nigga': 959,\n",
       " 'bitch fuck u': 960,\n",
       " 'bitch fuckin': 961,\n",
       " 'bitch funni': 962,\n",
       " 'bitch game': 963,\n",
       " 'bitch gave': 964,\n",
       " 'bitch gay': 965,\n",
       " 'bitch get': 966,\n",
       " 'bitch get mad': 967,\n",
       " 'bitch get money': 968,\n",
       " 'bitch gettin': 969,\n",
       " 'bitch give': 970,\n",
       " 'bitch go': 971,\n",
       " 'bitch goin': 972,\n",
       " 'bitch gold': 973,\n",
       " 'bitch gon': 974,\n",
       " 'bitch gon fuck': 975,\n",
       " 'bitch gone': 976,\n",
       " 'bitch gonna': 977,\n",
       " 'bitch good': 978,\n",
       " 'bitch got': 979,\n",
       " 'bitch gotta': 980,\n",
       " 'bitch guess': 981,\n",
       " 'bitch ha': 982,\n",
       " 'bitch haha': 983,\n",
       " 'bitch hair': 984,\n",
       " 'bitch hang': 985,\n",
       " 'bitch happi': 986,\n",
       " 'bitch hard': 987,\n",
       " 'bitch hate': 988,\n",
       " 'bitch havin': 989,\n",
       " 'bitch hella': 990,\n",
       " 'bitch hi': 991,\n",
       " 'bitch hit': 992,\n",
       " 'bitch hoe': 993,\n",
       " 'bitch hold': 994,\n",
       " 'bitch hope': 995,\n",
       " 'bitch hot': 996,\n",
       " 'bitch hous': 997,\n",
       " 'bitch huh': 998,\n",
       " \"bitch i'll\": 999,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 571)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    +\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA,\n",
    "                FRE,\n",
    "                syllables,\n",
    "                avg_syl,\n",
    "                num_chars,\n",
    "                num_chars_total,\n",
    "                num_terms,\n",
    "                num_words,\n",
    "                num_unique_terms,\n",
    "                sentiment['neg'],\n",
    "                sentiment['pos'],\n",
    "                sentiment['neu'],\n",
    "                sentiment['compound'],\n",
    "                twitter_objs[2],\n",
    "                twitter_objs[1],\n",
    "                twitter_objs[0],\n",
    "                retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\",\n",
    "                        \"FRE\",\n",
    "                        \"num_syllables\",\n",
    "                        \"avg_syl_per_word\",\n",
    "                        \"num_chars\",\n",
    "                        \"num_chars_total\",\n",
    "                        \"num_terms\",\n",
    "                        \"num_words\",\n",
    "                        \"num_unique_words\",\n",
    "                        \"vader neg\",\n",
    "                        \"vader pos\",\n",
    "                        \"vader neu\",\n",
    "                        \"vader compound\",\n",
    "                        \"num_hashtags\",\n",
    "                        \"num_mentions\",\n",
    "                        \"num_urls\",\n",
    "                        \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.8   73.15  30.   ...   1.     0.     0.  ]\n",
      " [  5.9   77.81  19.   ...   1.     0.     0.  ]\n",
      " [  6.5   80.46  23.   ...   2.     0.     1.  ]\n",
      " ...\n",
      " [  3.1   96.03  15.   ...   0.     0.     0.  ]\n",
      " [  0.6  103.05   8.   ...   0.     0.     0.  ]\n",
      " [  9.8   55.22  27.   ...   0.     1.     0.  ]]\n"
     ]
    }
   ],
   "source": [
    "feats = get_feature_array(tweets)\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 7859)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "This model was found using a GridSearch with 5-fold cross validation. Details are in the notebook operational_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of SelectFromModel(estimator=LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.56      0.49      1430\n",
      "           1       0.97      0.89      0.93     19190\n",
      "           2       0.76      0.95      0.85      4163\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     24783\n",
      "   macro avg       0.72      0.80      0.76     24783\n",
      "weighted avg       0.90      0.88      0.89     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using information from the model to obtain the matrix X_ generically\n",
    "\n",
    "This is the most difficult task: We have to take the inputs tweets and transform them into a format that can be used in the model without going through all the same pre-processing steps as above. This can be done as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = select.get_support(indices=True) #get indices of features\n",
    "final_feature_list = [str(feature_names[i]) for i in final_features] #Get list of names corresponding to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"bitch', '\"bitch\"', '\"nigger\"', '#faggot', '#tcot', '#yanke', '-', '2', 'activ', \"ain't\", 'al', 'america', 'american', 'anoth', 'ape', 'ass', 'ass hoe', 'ass nigga', 'bad', 'beaner', 'big', 'bird', 'bitch', 'bitch nigga', 'bitch!', 'bitch\"', 'black', 'border', 'bout', 'browni', 'buy', 'campu', 'charli', 'chill', 'chink', 'chug', 'clam', 'color', 'color folk', 'coon', 'countri', 'cracker', 'crazi', 'crow', 'cunt', 'da', 'damn', 'darki', 'dick', 'die', 'doe', 'dyke', 'dyke bitch', 'fag', 'faggot', 'fat', 'femal', 'feminist', 'filth', 'first', 'folk', 'fucc nicca', 'fuck', 'fuckin', 'fuzzi', 'game', 'gay', 'girl', 'go', 'good', 'gook', 'got', 'got nigga', 'hate', 'hate hoe', 'head', 'hi', 'ho', 'hoe', 'homo', 'hood', 'human', \"i'm\", 'jew', 'jig', 'jihadi', 'kill', 'lame', 'latina', 'least', 'let', 'like', 'lmfao', 'lol', 'look like', 'love', 'may', 'mention', 'mexican', 'mock', 'money', 'monkey', 'muslim', 'muzzi', 'negro', 'nicca', 'nig', 'nigga', 'nigga tri', 'niggah', 'niggaz', 'nigger', 'nigguh', 'night', 'niglet', 'oreo', 'peopl', 'play', 'pussi', 'pussi boy', 'pussy\"', 'put', 'queer', 'race', 'racist', 'real nigga', 'redneck', 'redskin', 'retard', \"say you'r\", 'shit', 'shoot', 'show', 'shut', 'slit', 'slope', 'smh', 'sole', 'speak', 'special', 'spic', 'stfu', 'stupid', 'suck', 'talk', 'teabagg', 'teach', 'thank', \"that'\", 'think', 'tho', 'towel', 'trailer', 'trash', 'tri', 'twat', 'u', 'ugli', 'uncl', 'ur', 'us', 'use', 'via', 'wa', 'wassup', 'wear', 'wetback', 'whi', 'white', 'white trash', 'whitey', 'whore', 'word', \"y'all\", 'ya', 'yanke', 'yeah', 'yellow', \"you'r\", 'zebra', 'zimmerman', '|', 'FW', 'JJ JJ', 'JJ NN JJ', 'JJ VBP', 'NN DT JJ', 'NN DT NN', 'NNP', 'VBD DT NN', 'VBP JJ', 'FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls']\n"
     ]
    }
   ],
   "source": [
    "print(final_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting names for each class of features\n",
    "\n",
    "# ngram_features = final_feature_list[:final_feature_list.index('zimmerman')+ 1]\n",
    "# pos_features = final_feature_list[final_feature_list.index('zimmerman')+1:final_feature_list.index('VBZ DT JJ')+1]\n",
    "# oth_features = final_feature_list[final_feature_list.index('FKRA'):]\n",
    "\n",
    "ngram_features = final_feature_list[:final_feature_list.index('zimmerman')+ 1]\n",
    "pos_features = final_feature_list[final_feature_list.index('zimmerman')+1:final_feature_list.index('VBP JJ')+1]\n",
    "oth_features = final_feature_list[final_feature_list.index('FKRA'):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram features: 181\n",
      "pos features: 10\n",
      "other features: 14\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print(f'ngram features: {len(ngram_features)}')\n",
    "print(f'pos features: {len(pos_features)}')\n",
    "print(f'other features: {len(oth_features)}')\n",
    "print(len(oth_features)+len(pos_features)+len(ngram_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {v:i for i, v in enumerate(ngram_features)}\n",
    "new_vocab_to_index = {}\n",
    "for k in ngram_features:\n",
    "    new_vocab_to_index[k] = vocab[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of text features\n",
    "ngram_indices = final_features[:len(ngram_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle new vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_vocab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# joblib.dump(new_vectorizer, 'py3models/final_tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24783, 181)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ = new_vectorizer.fit_transform(tweets).toarray()\n",
    "tfidf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 4.84692478, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 2.85845502, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.705379799270522"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,:tfidf_.shape[1]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are the same if use IDF but the problem is that IDF will be different if we use different data. Instead we have to use the original IDF scores and multiply them by the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vals_ = idf_vals[ngram_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle idf_vals\n",
    "\n",
    "# joblib.dump(idf_vals_, 'py3models/final_idf.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tfidf_[1,:]*idf_vals_) == X_[1,:153] #Got same value as final process array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_*idf_vals_ == X_[:,:153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 181)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidffinal = tfidf_*idf_vals_\n",
    "tfidffinal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating POS features\n",
    "This is simpler as we do not need to worry about IDF but it will be slower as we have to compute the POS tags for the new data. Here we can simply use the old POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = {v:i for i, v in enumerate(pos_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Pickle pos vectorizer\n",
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "new_pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    vocabulary=new_pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(new_pos_vectorizer, 'py3models/final_pos.pkl') \n",
    "# joblib.dump(model, 'py3models/final_mdl.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ = new_pos_vectorizer.fit_transform(tweet_tags).toarray()\n",
    "pos_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 2., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[1,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:] == X_[:,153:159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92309.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_[:,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13990.000798848323"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[:,153:159].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can look at the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neg', 'vader pos', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls', 'is_retweet']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(other_features_names)\n",
    "\n",
    "print(len(other_features_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FKRA', 'FRE', 'num_syllables', 'avg_syl_per_word', 'num_chars', 'num_chars_total', 'num_terms', 'num_words', 'num_unique_words', 'vader neu', 'vader compound', 'num_hashtags', 'num_mentions', 'num_urls']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(oth_features)\n",
    "print(len(oth_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FKRA\n",
      "FRE\n",
      "num_syllables\n",
      "avg_syl_per_word\n",
      "num_chars\n",
      "num_chars_total\n",
      "num_terms\n",
      "num_words\n",
      "num_unique_words\n",
      "vader neu\n",
      "vader compound\n",
      "num_hashtags\n",
      "num_mentions\n",
      "num_urls\n"
     ]
    }
   ],
   "source": [
    "for x in oth_features:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions can be modified to only calculate and return necessary fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA,\n",
    "                FRE,\n",
    "                syllables,\n",
    "                avg_syl,\n",
    "                num_chars,\n",
    "                num_chars_total,\n",
    "                num_terms,\n",
    "                num_words,\n",
    "                num_unique_terms,\n",
    "                sentiment['neu'],\n",
    "                sentiment['compound'],\n",
    "                twitter_objs[2],\n",
    "                twitter_objs[1],\n",
    "                twitter_objs[0]]\n",
    "\n",
    "    #features = pandas.DataFrame(features)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_feature_array_(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 14)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_ = get_feature_array_(tweets)\n",
    "feats_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.8   ,  73.15  ,  30.    ,   1.3043, 120.    , 140.    ,\n",
       "        25.    ,  23.    ,  21.    ,   0.88  ,   0.4563,   0.    ,\n",
       "         1.    ,   0.    ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.    ,   0.    ,   0.    ,   0.    ,   0.    ,   0.    ,\n",
       "         0.    ,   0.    ,   0.    ,   3.    ,   0.    ,   0.    ,\n",
       "         1.    ,   4.    ,   0.    ,   1.    ,   0.    ,   8.8   ,\n",
       "        73.15  ,  30.    ,   1.3043, 120.    , 140.    ,  25.    ,\n",
       "        23.    ,  21.    ,   0.88  ,   0.4563,   0.    ,   1.    ,\n",
       "         0.    ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,174:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_[:,:] == X_[:,159:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have put it all together using a simplified process we can assess if these new data return the same answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ = np.concatenate([tfidffinal, pos_, feats_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 205)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X__ = pd.DataFrame(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_ = model.predict(X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.56      0.49      1430\n",
      "           1       0.97      0.89      0.93     19190\n",
      "           2       0.76      0.95      0.85      4163\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     24783\n",
      "   macro avg       0.72      0.80      0.76     24783\n",
      "weighted avg       0.90      0.88      0.89     24783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So now that we have verified that the results are the same with X_ and X__ we can implement a script that can transform new data in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 205)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['py3models/final_tfidf2.pkl']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(idf_vals_, 'py3models/final_idf2.pkl')\n",
    "joblib.dump(new_pos_vectorizer, 'py3models/final_pos2.pkl') \n",
    "joblib.dump(model, 'py3models/final_mdl2.pkl')\n",
    "joblib.dump(new_vectorizer, 'py3models/final_tfidf2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
