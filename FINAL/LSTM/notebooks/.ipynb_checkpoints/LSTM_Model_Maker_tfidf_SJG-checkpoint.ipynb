{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import seaborn as sns\n",
    "import shap # for SHAP value\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Use Keras Tensorflow deeplearning library\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import eli5 # for permutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from pdpbox import pdp, info_plots # flor partial plots \n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_csv(\"input/train.csv\")\n",
    "train2 = train2.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "train2 = train2.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()\n",
    "\n",
    "target_t2 = train2.Sentiment.values\n",
    "y_target_t2 = to_categorical(target_t2)\n",
    "\n",
    "# Save the texts before tokenizing (must use the same random seed)\n",
    "X_train_t2, X_val_t2, y_train_t2, y_val_t2 = train_test_split(train2,y_target_t2,\n",
    "                                                              test_size=0.2,\n",
    "                                                              random_state=123,\n",
    "                                                              stratify=y_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset: Need only class as \"Sentiment\" and text as 'Phrase'\n",
    "train = train.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "# train = train.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to Save the split texts before cleaning and tokenizing\n",
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target_t = train.Sentiment.values\n",
    "y_target_t = to_categorical(target_t)\n",
    "\n",
    "# Save the texts before tokenizing (must use the same random seed)\n",
    "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(train,y_target_t,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42,\n",
    "                                                          stratify=y_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    tweets = []\n",
    "    \n",
    "#     for sent in tqdm(df['Phrase']):\n",
    "    for sent in df['Phrase']:\n",
    "        # remove non-alphabetic characters\n",
    "        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(sent))\n",
    "        \n",
    "        #remove html content\n",
    "        tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        words = word_tokenize(tweet_text.lower())\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        tweets.append(lemma_words)\n",
    "        \n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned tweets for both train and test set retrieved\n",
    "\n",
    "train_sentences = clean_sentences(train)\n",
    "\n",
    "test_sentences = clean_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target = train.Sentiment.values\n",
    "y_target = to_categorical(target)\n",
    "num_classes = y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sentences,\n",
    "                                                  y_target,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28701\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Getting the no of unique words and max length of a tweet available in the list of cleaned tweets\n",
    "# It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "# Build an unordered collection of unique elements.\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "# for sent in tqdm(X_train):\n",
    "for sent in X_train:\n",
    "\n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max=len(sent)\n",
    "\n",
    "# length of the list of unique_words gives the number of unique words\n",
    "\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual tokenizer of keras and convert to sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# texts_to_sequences\n",
    "# ARGUMENTS: list of texts to turn to sequences\n",
    "# RETURN: list of sequences (one per text input)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is done to equalize the lengths of all input tweets.\n",
    "# LTSM networks need all inputs to be same length.\n",
    "# Therefore, tweets lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "# Also, you always have to give a three-dimensional array as an input to your LSTM network\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting deep learning neural network models\n",
    "# This is a method that allows you to specify an arbitrary large number of training epochs.\n",
    "# This stops training once the model performance stops improving on a hold out validation dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor = 'val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 16:45:01.245801 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0802 16:45:01.249207 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 16:45:01.252202 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0802 16:45:01.332825 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0802 16:45:01.339013 4617016768 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0802 16:45:01.967109 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0802 16:45:01.987466 4617016768 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 53, 300)           8610300   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,886,159\n",
      "Trainable params: 8,886,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with just acc as metric\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu')) #try elu\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 53, 300)           8610300   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,886,159\n",
      "Trainable params: 8,886,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with no metrics\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model_1.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model_1.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model_1.add(Dense(100,activation='relu')) #try elu\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes,activation='softmax'))\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1144.0\n",
       "1    15348.0\n",
       "2     3330.0\n",
       "dtype: float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas df for y_train\n",
    "# Describe classifier hate (0), offensive (1), neither or neutral (2)\n",
    "\n",
    "y_df = pd.DataFrame(y_train)\n",
    "y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the no. of records for each classifier\n",
    "hate_num = y_df[0].sum()\n",
    "offensive_num = y_df[1].sum()\n",
    "neutral_num = y_df[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K Hate:0.9422863485016648\n",
      " S Hate:0.9422863485016648\n",
      "K neutral: 0.8320048431036222\n",
      "S neutral: 0.8320048431036222\n",
      "K Offensive: 0.22570880839471297\n",
      "S Offensive: 0.22570880839471297\n"
     ]
    }
   ],
   "source": [
    "# Do inverse ratio so that hate and neutral has a higher weight\n",
    "print(f' K Hate:{1 - hate_num / len(y_df)}')\n",
    "print(f' S Hate:{1 - (hate_num / len(y_df))}')\n",
    "\n",
    "print(f'K neutral: {1 - neutral_num / len(y_df)}')\n",
    "print(f'S neutral: {1 - (neutral_num / len(y_df))}')\n",
    "\n",
    "# Inverse ratio will give offensive a lower weight\n",
    "print(f'K Offensive: {1 - offensive_num / len(y_df)}')\n",
    "print(f'S Offensive: {1 - (offensive_num / len(y_df))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 16:45:02.742547 4617016768 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19822 samples, validate on 4956 samples\n",
      "Epoch 1/15\n",
      "19822/19822 [==============================] - 46s 2ms/step - loss: 4.9290 - acc: 0.0648 - val_loss: 3.2425 - val_acc: 0.0577\n",
      "Epoch 2/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 3.2429 - acc: 0.1268 - val_loss: 3.2940 - val_acc: 0.1663\n",
      "Epoch 3/15\n",
      "19822/19822 [==============================] - 45s 2ms/step - loss: 1.5404 - acc: 0.2049 - val_loss: 2.7059 - val_acc: 0.1957\n",
      "Epoch 4/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.9828 - acc: 0.2186 - val_loss: 2.6040 - val_acc: 0.2002\n",
      "Epoch 5/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.7032 - acc: 0.2469 - val_loss: 1.6988 - val_acc: 0.3093\n",
      "Epoch 6/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.5224 - acc: 0.5431 - val_loss: 1.4557 - val_acc: 0.5682\n",
      "Epoch 7/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.3977 - acc: 0.7081 - val_loss: 1.3459 - val_acc: 0.6421\n",
      "Epoch 8/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.3269 - acc: 0.7622 - val_loss: 1.2895 - val_acc: 0.7014\n",
      "Epoch 9/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.2779 - acc: 0.7826 - val_loss: 1.3594 - val_acc: 0.6624\n",
      "Epoch 10/15\n",
      "19822/19822 [==============================] - 47s 2ms/step - loss: 0.2552 - acc: 0.7929 - val_loss: 1.1120 - val_acc: 0.7042\n",
      "Epoch 11/15\n",
      "19822/19822 [==============================] - 51s 3ms/step - loss: 0.2007 - acc: 0.8480 - val_loss: 1.2541 - val_acc: 0.7100\n",
      "Epoch 12/15\n",
      "19822/19822 [==============================] - 41s 2ms/step - loss: 0.1683 - acc: 0.8746 - val_loss: 1.2120 - val_acc: 0.7205\n",
      "Epoch 13/15\n",
      "19822/19822 [==============================] - 41s 2ms/step - loss: 0.1493 - acc: 0.8956 - val_loss: 1.5719 - val_acc: 0.6717\n",
      "Epoch 14/15\n",
      "19822/19822 [==============================] - 41s 2ms/step - loss: 0.1458 - acc: 0.8849 - val_loss: 1.3474 - val_acc: 0.7046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c43139198>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model adjusting for epochs, batch, and weight\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: .94*100, 1: .23, 2: .83*10} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19822 samples, validate on 4956 samples\n",
      "Epoch 1/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 4.8948 - val_loss: 3.4851\n",
      "Epoch 2/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 4.1251 - val_loss: 2.7961\n",
      "Epoch 3/15\n",
      "19822/19822 [==============================] - 46s 2ms/step - loss: 2.0300 - val_loss: 2.6656\n",
      "Epoch 4/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 1.1129 - val_loss: 2.1603\n",
      "Epoch 5/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.7396 - val_loss: 2.1422\n",
      "Epoch 6/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.5227 - val_loss: 1.2960\n",
      "Epoch 7/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.3864 - val_loss: 1.8326\n",
      "Epoch 8/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.3776 - val_loss: 1.1942\n",
      "Epoch 9/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.2480 - val_loss: 1.2350\n",
      "Epoch 10/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.2802 - val_loss: 1.1710\n",
      "Epoch 11/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.2002 - val_loss: 1.2824\n",
      "Epoch 12/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.1410 - val_loss: 1.0002\n",
      "Epoch 13/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.1868 - val_loss: 1.3517\n",
      "Epoch 14/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.1916 - val_loss: 1.1926\n",
      "Epoch 15/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 0.1608 - val_loss: 1.0690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c43114748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: .94*100, 1: .23, 2: .83*10} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation sentiment!\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "# print(y_pred)\n",
    "\n",
    "y_pred_1 = model_1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and Sklearn read arrays differently\n",
    "# Create function to convert keras array to show only one highest sentiment result per list\n",
    "def keras_output_sklearn(y):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for element in y:\n",
    "        result.append(np.argmax(element))\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.69      0.26       286\n",
      "           1       0.96      0.67      0.79      3838\n",
      "           2       0.68      0.89      0.77       832\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      4956\n",
      "   macro avg       0.60      0.75      0.61      4956\n",
      "weighted avg       0.87      0.70      0.75      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report for model with 'acc' metric\n",
    "report = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.63      0.30       286\n",
      "           1       0.96      0.72      0.82      3838\n",
      "           2       0.65      0.91      0.76       832\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      4956\n",
      "   macro avg       0.60      0.75      0.63      4956\n",
      "weighted avg       0.86      0.75      0.78      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report for model with no metrics \n",
    "report_1 = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1))\n",
    "print(report_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model with 'acc' metric\n",
    "model.save('../models/model_acc.h5')\n",
    "\n",
    "# Saving model with no metrics\n",
    "model_1.save('../models/model_no.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
