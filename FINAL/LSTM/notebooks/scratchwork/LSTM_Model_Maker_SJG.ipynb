{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import seaborn as sns\n",
    "import shap # for SHAP value\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Use Keras Tensorflow deeplearning library\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import eli5 # for permutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from pdpbox import pdp, info_plots # flor partial plots \n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from joblib import dump, load\n",
    "seed_value = 123\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset: Need only class as \"Sentiment\" and text as 'Phrase'\n",
    "train = train.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "train = train.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset: Need only text as 'Phrase'\n",
    "test = test.rename(columns={'Text':'Phrase'})\n",
    "test = test.drop(['Date', 'Favorites', 'Retweets', 'Tweet ID'],axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to Save the split texts before cleaning and tokenizing\n",
    "# # Collect dependent values and convert to ONE-HOT encoding\n",
    "# # Output using to_categorical\n",
    "# target_t = train.Sentiment.values\n",
    "# y_target_t = to_categorical(target_t)\n",
    "\n",
    "# # Save the texts before tokenizing (must use the same random seed)\n",
    "# X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(train,y_target_t,\n",
    "#                                                           test_size=0.2,\n",
    "#                                                           random_state=seed_value,\n",
    "#                                                           stratify=y_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    tweets = []\n",
    "    \n",
    "#     for sent in tqdm(df['Phrase']):\n",
    "    for sent in df['Phrase']:\n",
    "        # remove non-alphabetic characters\n",
    "        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(sent))\n",
    "        \n",
    "        #remove html content\n",
    "        tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        words = word_tokenize(tweet_text.lower())\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        tweets.append(lemma_words)\n",
    "        \n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned tweets for both train and test set retrieved\n",
    "\n",
    "train_sentences = clean_sentences(train)\n",
    "\n",
    "test_sentences = clean_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target = train.Sentiment.values\n",
    "y_target = to_categorical(target)\n",
    "num_classes = y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sentences,\n",
    "                                                  y_target,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=seed_value,\n",
    "                                                  stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28804\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Getting the no of unique words and max length of a tweet available in the list of cleaned tweets\n",
    "# It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "# Build an unordered collection of unique elements.\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "# for sent in tqdm(X_train):\n",
    "for sent in X_train:\n",
    "\n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max=len(sent)\n",
    "\n",
    "# length of the list of unique_words gives the number of unique words\n",
    "\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual tokenizer of keras and convert to sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# texts_to_sequences\n",
    "# ARGUMENTS: list of texts to turn to sequences\n",
    "# RETURN: list of sequences (one per text input)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is done to equalize the lengths of all input tweets.\n",
    "# LTSM networks need all inputs to be same length.\n",
    "# Therefore, tweets lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "# Also, you always have to give a three-dimensional array as an input to your LSTM network\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting deep learning neural network models\n",
    "# This is a method that allows you to specify an arbitrary large number of training epochs.\n",
    "# This stops training once the model performance stops improving on a hold out validation dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor = 'val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 18:16:04.904451 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0803 18:16:04.907301 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 18:16:04.909843 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0803 18:16:04.987577 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0803 18:16:04.994526 4642133440 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0803 18:16:05.624413 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0803 18:16:05.644890 4642133440 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 53, 300)           8641200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,917,059\n",
      "Trainable params: 8,917,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with just acc as metric\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu')) #try elu\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 53, 300)           8641200   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,917,059\n",
      "Trainable params: 8,917,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with no metrics\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model_1.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model_1.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model_1.add(Dense(100,activation='relu')) #try elu\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes,activation='softmax'))\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1144.0\n",
       "1    15348.0\n",
       "2     3330.0\n",
       "dtype: float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas df for y_train\n",
    "# Describe classifier hate (0), offensive (1), neither or neutral (2)\n",
    "\n",
    "y_df = pd.DataFrame(y_train)\n",
    "y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of number of data points in each category\n",
    "hate_ct = y_train.sum(axis = 0)[0]\n",
    "offensive_ct = y_train.sum(axis = 0)[1]\n",
    "neither_ct = y_train.sum(axis = 0)[2]\n",
    "total_ct = y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the inverse ratio of each category to use for the weights of the model\n",
    "inv_ratio_hate = 1 - (hate_ct / total_ct)\n",
    "inv_ratio_hurtful = 1 - (offensive_ct / total_ct)\n",
    "inv_ratio_neither = 1 - (neither_ct / total_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the no. of records for each classifier\n",
    "hate_num = y_df[0].sum()\n",
    "offensive_num = y_df[1].sum()\n",
    "neutral_num = y_df[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K Hate:0.9422863485016648\n",
      " S Hate:0.9422863485016648\n",
      "K neutral: 0.8320048431036222\n",
      "S neutral: 0.8320048431036222\n",
      "K Offensive: 0.22570880839471297\n",
      "S Offensive: 0.22570880839471297\n"
     ]
    }
   ],
   "source": [
    "# Do inverse ratio so that hate and neutral has a higher weight\n",
    "print(f' K Hate:{1 - hate_num / len(y_df)}')\n",
    "print(f' S Hate:{1 - (hate_num / len(y_df))}')\n",
    "\n",
    "print(f'K neutral: {1 - neutral_num / len(y_df)}')\n",
    "print(f'S neutral: {1 - (neutral_num / len(y_df))}')\n",
    "\n",
    "# Inverse ratio will give offensive a lower weight\n",
    "print(f'K Offensive: {1 - offensive_num / len(y_df)}')\n",
    "print(f'S Offensive: {1 - (offensive_num / len(y_df))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19822 samples, validate on 4956 samples\n",
      "Epoch 1/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.1370 - acc: 0.9025 - val_loss: 0.3304 - val_acc: 0.8816\n",
      "Epoch 2/15\n",
      "19822/19822 [==============================] - 45s 2ms/step - loss: 0.0935 - acc: 0.9215 - val_loss: 0.3651 - val_acc: 0.8745\n",
      "Epoch 3/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.0651 - acc: 0.9439 - val_loss: 0.4268 - val_acc: 0.8553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c447be550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model adjusting for epochs, batch, and weight\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: inv_ratio_hate,\n",
    "                  1: inv_ratio_hurtful,\n",
    "                  2: inv_ratio_neither} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19822 samples, validate on 4956 samples\n",
      "Epoch 1/15\n",
      "19822/19822 [==============================] - 47s 2ms/step - loss: 0.3204 - val_loss: 0.4069\n",
      "Epoch 2/15\n",
      "19822/19822 [==============================] - 57s 3ms/step - loss: 0.1732 - val_loss: 0.3392\n",
      "Epoch 3/15\n",
      "19822/19822 [==============================] - 53s 3ms/step - loss: 0.1135 - val_loss: 0.3615\n",
      "Epoch 4/15\n",
      "19822/19822 [==============================] - 51s 3ms/step - loss: 0.0807 - val_loss: 0.3872\n",
      "Epoch 5/15\n",
      "19822/19822 [==============================] - 61s 3ms/step - loss: 0.0568 - val_loss: 0.4540\n",
      "Epoch 6/15\n",
      "19822/19822 [==============================] - 129s 6ms/step - loss: 0.0434 - val_loss: 0.4688\n",
      "Epoch 7/15\n",
      "19822/19822 [==============================] - 66s 3ms/step - loss: 0.0330 - val_loss: 0.5105\n",
      "Epoch 8/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0254 - val_loss: 0.5859\n",
      "Epoch 9/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0203 - val_loss: 0.5695\n",
      "Epoch 10/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0170 - val_loss: 0.6187\n",
      "Epoch 11/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0174 - val_loss: 0.6717\n",
      "Epoch 12/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0139 - val_loss: 0.7368\n",
      "Epoch 13/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0137 - val_loss: 0.6915\n",
      "Epoch 14/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0114 - val_loss: 0.7256\n",
      "Epoch 15/15\n",
      "19822/19822 [==============================] - 39s 2ms/step - loss: 0.0109 - val_loss: 0.7781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c44820160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: inv_ratio_hate,\n",
    "                  1: inv_ratio_hurtful,\n",
    "                  2: inv_ratio_neither} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation sentiment!\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "# print(y_pred)\n",
    "\n",
    "y_pred_1 = model_1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and Sklearn read arrays differently\n",
    "# Create function to convert keras array to show only one highest sentiment result per list\n",
    "def keras_output_sklearn(y):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for element in y:\n",
    "        result.append(np.argmax(element))\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.50      0.38       286\n",
      "           1       0.93      0.89      0.91      3838\n",
      "           2       0.83      0.79      0.81       832\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      4956\n",
      "   macro avg       0.69      0.73      0.70      4956\n",
      "weighted avg       0.88      0.86      0.87      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35       286\n",
      "           1       0.92      0.92      0.92      3838\n",
      "           2       0.81      0.82      0.81       832\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      4956\n",
      "   macro avg       0.70      0.70      0.70      4956\n",
      "weighted avg       0.87      0.87      0.87      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Report for model with 'acc' metric\n",
    "# report = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred))\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report for model with no metrics \n",
    "report_1 = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1))\n",
    "print(report_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model with 'acc' metric\n",
    "model.save('../models/model_acc.h5')\n",
    "\n",
    "# Saving model with no metrics\n",
    "model_1.save('../models/model_no.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
