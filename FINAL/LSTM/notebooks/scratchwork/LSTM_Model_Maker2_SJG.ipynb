{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import seaborn as sns\n",
    "import shap # for SHAP value\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Use Keras Tensorflow deeplearning library\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc,make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import eli5 # for permutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from pdpbox import pdp, info_plots # flor partial plots \n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from joblib import dump, load\n",
    "seed_value = 123\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset: Need only class as \"Sentiment\" and text as 'Phrase'\n",
    "train = train.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "train = train.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset: Need only text as 'Phrase'\n",
    "test = test.rename(columns={'Text':'Phrase'})\n",
    "test = test.drop(['Date', 'Favorites', 'Retweets', 'Tweet ID'],axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to Save the split texts before cleaning and tokenizing\n",
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target_t = train.Sentiment.values\n",
    "y_target_t = to_categorical(target_t)\n",
    "\n",
    "# Save the texts before tokenizing (must use the same random seed)\n",
    "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(train,y_target_t,\n",
    "                                                          test_size=0.1,\n",
    "                                                          random_state=123,\n",
    "                                                          stratify=y_target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    tweets = []\n",
    "    \n",
    "#     for sent in tqdm(df['Phrase']):\n",
    "    for sent in df['Phrase']:\n",
    "        # remove non-alphabetic characters\n",
    "        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", str(sent))\n",
    "        \n",
    "        #remove html content\n",
    "        tweet_text = BeautifulSoup(tweet_text).get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        words = word_tokenize(tweet_text.lower())\n",
    "        \n",
    "        # lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        tweets.append(lemma_words)\n",
    "        \n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned tweets for both train and test set retrieved\n",
    "\n",
    "train_sentences = clean_sentences(train)\n",
    "\n",
    "test_sentences = clean_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dependent values and convert to ONE-HOT encoding\n",
    "# Output using to_categorical\n",
    "target = train.Sentiment.values\n",
    "y_target = to_categorical(target)\n",
    "num_classes = y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_sentences,\n",
    "                                                  y_target,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28701\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Getting the no of unique words and max length of a tweet available in the list of cleaned tweets\n",
    "# It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "# Build an unordered collection of unique elements.\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "# for sent in tqdm(X_train):\n",
    "for sent in X_train:\n",
    "\n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max=len(sent)\n",
    "\n",
    "# length of the list of unique_words gives the number of unique words\n",
    "\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual tokenizer of keras and convert to sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# texts_to_sequences\n",
    "# ARGUMENTS: list of texts to turn to sequences\n",
    "# RETURN: list of sequences (one per text input)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is done to equalize the lengths of all input tweets.\n",
    "# LTSM networks need all inputs to be same length.\n",
    "# Therefore, tweets lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "# Also, you always have to give a three-dimensional array as an input to your LSTM network\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting deep learning neural network models\n",
    "# This is a method that allows you to specify an arbitrary large number of training epochs.\n",
    "# This stops training once the model performance stops improving on a hold out validation dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor = 'val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 11:58:09.811493 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0803 11:58:09.817187 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 11:58:09.819895 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0803 11:58:09.915667 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0803 11:58:09.927006 4450584000 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0803 11:58:10.664112 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0803 11:58:10.685434 4450584000 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 53, 300)           8610300   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 53, 100)           12900     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 8,885,283\n",
      "Trainable params: 8,885,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with just acc as metric\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 53, 300)           8610300   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 53, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,886,159\n",
      "Trainable params: 8,886,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with no metrics\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model_1.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model_1.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model_1.add(Dense(100,activation='relu')) #try elu\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes,activation='softmax'))\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1144.0\n",
       "1    15348.0\n",
       "2     3330.0\n",
       "dtype: float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas df for y_train\n",
    "# Describe classifier hate (0), offensive (1), neither or neutral (2)\n",
    "\n",
    "y_df = pd.DataFrame(y_train)\n",
    "y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the no. of records for each classifier\n",
    "hate_num = y_df[0].sum()\n",
    "offensive_num = y_df[1].sum()\n",
    "neutral_num = y_df[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K Hate:0.9422863485016648\n",
      " S Hate:0.9422863485016648\n",
      "K neutral: 0.8320048431036222\n",
      "S neutral: 0.8320048431036222\n",
      "K Offensive: 0.22570880839471297\n",
      "S Offensive: 0.22570880839471297\n"
     ]
    }
   ],
   "source": [
    "# Do inverse ratio so that hate and neutral has a higher weight\n",
    "print(f' K Hate:{1 - hate_num / len(y_df)}')\n",
    "print(f' S Hate:{1 - (hate_num / len(y_df))}')\n",
    "\n",
    "print(f'K neutral: {1 - neutral_num / len(y_df)}')\n",
    "print(f'S neutral: {1 - (neutral_num / len(y_df))}')\n",
    "\n",
    "# Inverse ratio will give offensive a lower weight\n",
    "print(f'K Offensive: {1 - offensive_num / len(y_df)}')\n",
    "print(f'S Offensive: {1 - (offensive_num / len(y_df))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 11:58:11.739079 4450584000 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 6656/19822 [=========>....................] - ETA: 32s - loss: 5.2495 - acc: 0.0759"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-805bb82fc7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#stops training once the model stops improving. Prevents overfitting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.94\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.83\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#use inverse ratio to set hate with highest weight (somewhat arbitrary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m ) \n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model adjusting for epochs, batch, and weight\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "#    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: .94*100, 1: .23, 2: .83*10} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19822 samples, validate on 4956 samples\n",
      "Epoch 1/15\n",
      "19822/19822 [==============================] - 49s 2ms/step - loss: 4.8657 - acc: 0.0672 - val_loss: 3.4696 - val_acc: 0.0577\n",
      "Epoch 2/15\n",
      "19822/19822 [==============================] - 46s 2ms/step - loss: 3.1487 - acc: 0.1309 - val_loss: 2.7525 - val_acc: 0.1885\n",
      "Epoch 3/15\n",
      "19822/19822 [==============================] - 43s 2ms/step - loss: 1.4560 - acc: 0.2087 - val_loss: 2.5317 - val_acc: 0.2030\n",
      "Epoch 4/15\n",
      "19822/19822 [==============================] - 48s 2ms/step - loss: 0.9970 - acc: 0.2184 - val_loss: 2.2588 - val_acc: 0.2060\n",
      "Epoch 5/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.7023 - acc: 0.2259 - val_loss: 2.0932 - val_acc: 0.2179\n",
      "Epoch 6/15\n",
      "19822/19822 [==============================] - 44s 2ms/step - loss: 0.5327 - acc: 0.3662 - val_loss: 2.1471 - val_acc: 0.3535\n",
      "Epoch 7/15\n",
      "19822/19822 [==============================] - 42s 2ms/step - loss: 0.4324 - acc: 0.5958 - val_loss: 1.6161 - val_acc: 0.5256\n",
      "Epoch 8/15\n",
      "19822/19822 [==============================] - 49s 2ms/step - loss: 0.3276 - acc: 0.7117 - val_loss: 1.5539 - val_acc: 0.6051\n",
      "Epoch 9/15\n",
      "19822/19822 [==============================] - 48s 2ms/step - loss: 0.2642 - acc: 0.7807 - val_loss: 1.3569 - val_acc: 0.6612\n",
      "Epoch 10/15\n",
      "19822/19822 [==============================] - 49s 2ms/step - loss: 0.2307 - acc: 0.8254 - val_loss: 1.1417 - val_acc: 0.7274\n",
      "Epoch 11/15\n",
      "19822/19822 [==============================] - 50s 3ms/step - loss: 0.1866 - acc: 0.8608 - val_loss: 1.4652 - val_acc: 0.6927\n",
      "Epoch 12/15\n",
      "19822/19822 [==============================] - 50s 3ms/step - loss: 0.1623 - acc: 0.8714 - val_loss: 1.1515 - val_acc: 0.7480\n",
      "Epoch 13/15\n",
      "19822/19822 [==============================] - 48s 2ms/step - loss: 0.1449 - acc: 0.8864 - val_loss: 1.0568 - val_acc: 0.7704\n",
      "Epoch 14/15\n",
      "19822/19822 [==============================] - 52s 3ms/step - loss: 0.1358 - acc: 0.8892 - val_loss: 1.2267 - val_acc: 0.7534\n",
      "Epoch 15/15\n",
      "19822/19822 [==============================] - 50s 3ms/step - loss: 0.1622 - acc: 0.8717 - val_loss: 1.3039 - val_acc: 0.7026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c46663a58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val,y_val),\n",
    "    epochs=15, #may not run all due to callback\n",
    "    batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "    verbose=1,\n",
    "    callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "    class_weight={0: .94*100, 1: .23, 2: .83*10} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation sentiment!\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "# print(y_pred)\n",
    "\n",
    "y_pred_1 = model_1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and Sklearn read arrays differently\n",
    "# Create function to convert keras array to show only one highest sentiment result per list\n",
    "def keras_output_sklearn(y):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for element in y:\n",
    "        result.append(np.argmax(element))\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.68      0.28       286\n",
      "           1       0.96      0.63      0.76      3838\n",
      "           2       0.56      0.91      0.70       832\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      4956\n",
      "   macro avg       0.57      0.74      0.58      4956\n",
      "weighted avg       0.85      0.68      0.72      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report for model with 'acc' metric\n",
    "report = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.66      0.28       286\n",
      "           1       0.96      0.66      0.78      3838\n",
      "           2       0.60      0.92      0.73       832\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      4956\n",
      "   macro avg       0.58      0.74      0.60      4956\n",
      "weighted avg       0.86      0.70      0.74      4956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report for model with no metrics \n",
    "report_1 = classification_report(keras_output_sklearn(y_val), keras_output_sklearn(y_pred_1))\n",
    "print(report_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model with 'acc' metric\n",
    "model.save('../models2/model_acc.h5')\n",
    "\n",
    "# Saving model with no metrics\n",
    "model_1.save('../models2/model_no.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.history.history\n",
    "\n",
    "# X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(train,y_target_t,\n",
    "#                                                           test_size=0.2,\n",
    "#                                                           random_state=42,\n",
    "#                                                           stratify=y_target_t)\n",
    "\n",
    "y_train_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_csv(\"input/train.csv\")\n",
    "train2 = train2.rename(columns={'class':'Sentiment','tweet':'Phrase'})\n",
    "train2 = train2.drop(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'],axis=1).copy()\n",
    "\n",
    "target_t2 = train2.Sentiment.values\n",
    "y_target_t2 = to_categorical(target_t2)\n",
    "\n",
    "# Save the texts before tokenizing (must use the same random seed)\n",
    "X_train_t2, X_val_t2, y_train_t2, y_val_t2 = train_test_split(train2,y_target_t2,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=123,\n",
    "                                                          stratify=y_target_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16906</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @PluggedInPiff: @ThoughtsOfRandy stop follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23020</th>\n",
       "      <td>1</td>\n",
       "      <td>Y'all bitches wanna be so different and y'all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SaNtAnAdaGrEaT: I never understood that &amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>0</td>\n",
       "      <td>Brady heard yall say he was washed, now he was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23303</th>\n",
       "      <td>0</td>\n",
       "      <td>You fucking fag &amp;#8220;@baethingape: this is e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                             Phrase\n",
       "16906          1  RT @PluggedInPiff: @ThoughtsOfRandy stop follo...\n",
       "23020          1  Y'all bitches wanna be so different and y'all ...\n",
       "17176          1  RT @SaNtAnAdaGrEaT: I never understood that &#...\n",
       "8163           0  Brady heard yall say he was washed, now he was...\n",
       "23303          0  You fucking fag &#8220;@baethingape: this is e..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_t2.Phrase\n",
    "\n",
    "# X_train_t2.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = X_train_t2.Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stephengriggs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/stephengriggs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "nltk.download('punkt')    \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "## Omari's version\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "       '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-\\:]+' #<<<<<<added the semicolon after the + to remove : at end of Rt's\n",
    "    emoji_regex = '&#[0-9\\;\\:]+'    #<<<<<<<<<remove emoji's .ex; &#1214324\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(emoji_regex,'',parsed_text)\n",
    "    parsed_text = parsed_text.strip(string.punctuation)\n",
    "    return parsed_text\n",
    "\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split('\\s|(?<!\\d)[,.]|[,.](?!\\d)', tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19822, 5838)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19822, 542)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    +\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA,\n",
    "                FRE,\n",
    "                syllables,\n",
    "                avg_syl,\n",
    "                num_chars,\n",
    "                num_chars_total,\n",
    "                num_terms,\n",
    "                num_words,\n",
    "                num_unique_terms,\n",
    "                sentiment['neg'],\n",
    "                sentiment['pos'],\n",
    "                sentiment['neu'],\n",
    "                sentiment['compound'],\n",
    "                twitter_objs[2],\n",
    "                twitter_objs[1],\n",
    "                twitter_objs[0],\n",
    "                retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\",\n",
    "                        \"FRE\",\n",
    "                        \"num_syllables\",\n",
    "                        \"avg_syl_per_word\",\n",
    "                        \"num_chars\",\n",
    "                        \"num_chars_total\",\n",
    "                        \"num_terms\",\n",
    "                        \"num_words\",\n",
    "                        \"num_unique_words\",\n",
    "                        \"vader neg\",\n",
    "                        \"vader pos\",\n",
    "                        \"vader neu\",\n",
    "                        \"vader compound\",\n",
    "                        \"num_hashtags\",\n",
    "                        \"num_mentions\",\n",
    "                        \"num_urls\",\n",
    "                        \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19822, 17)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = get_feature_array(tweets)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19822, 6397)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.concatenate([tfidf,pos,feats],axis=1)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor = 'val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "754\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "# for sent in tqdm(X_train):\n",
    "for sent in tweets:\n",
    "\n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max=len(sent)\n",
    "\n",
    "# length of the list of unique_words gives the number of unique words\n",
    "\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6397"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "(19822, 53)\n"
     ]
    }
   ],
   "source": [
    "print(len(list(unique_words)))\n",
    "print(X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 6397, 300)         28200     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 6397, 128)         219648    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 304,059\n",
      "Trainable params: 304,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# re-run model everytime new parameter changes (must run sequential to initialize epoch)\n",
    "# Model with just acc as metric\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(len(list(unique_words)),300,input_length=M.shape[1]))\n",
    "model_2.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
    "model_2.add(LSTM(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\n",
    "model_2.add(Dense(100,activation='relu')) #try elu\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes,activation='softmax'))\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2.fit(\n",
    "#     M, y_train_t2, \n",
    "#     #validation_data=(X_val,y_val),\n",
    "#     epochs=15, #may not run all due to callback\n",
    "#     batch_size=256, #faster with larger batch_size but it's generalizing\n",
    "#     verbose=1,\n",
    "#     callbacks=callback, #stops training once the model stops improving. Prevents overfitting.\n",
    "# #    class_weight={0: .94*100, 1: .23, 2: .83*10} #use inverse ratio to set hate with highest weight (somewhat arbitrary)\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7849359 , 0.02112642, 0.19393778],\n",
       "       [0.76216084, 0.02982378, 0.20801541],\n",
       "       [0.7733667 , 0.02328585, 0.20334744],\n",
       "       ...,\n",
       "       [0.7530709 , 0.03221582, 0.21471332],\n",
       "       [0.75436056, 0.0321478 , 0.2134916 ],\n",
       "       [0.7680189 , 0.02712601, 0.20485508]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
